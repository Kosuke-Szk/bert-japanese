{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of the pretrained Japanese BERT model\n",
    "\n",
    "Finetune the pretrained model to solve multi-class classification problems.  \n",
    "This notebook requires the following objects:\n",
    "- trained sentencepiece model (model and vocab files)\n",
    "- pretraiend Japanese BERT model\n",
    "\n",
    "We make test:dev:train = 2:2:6 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/workspace/bert-japanese/notebook/../config.ini']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "CURDIR = os.getcwd()\n",
    "CONFIGPATH = os.path.join(CURDIR, os.pardir, 'config.ini')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIGPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparing\n",
    "\n",
    "You need execute the following cells just once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEURL = config['HIS-DATA']['DATADIR']\n",
    "EXTRACTDIR = config['HIS-DATA']['TEXTDIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEURL, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532</td>\n",
       "      <td>ソウルの渡航条件は？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>今のおススメはある？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>主人と別々に会員登録したいんですけど、２人のメアドがいっしょでもできますか？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>海外お土産や旅行用品を希望の日時に郵送してもらいたいのですが。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>397</td>\n",
       "      <td>旅行のキャンセル料が、旅行代金より高くなることはありますか？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                    text\n",
       "0    532                              ソウルの渡航条件は？\n",
       "1    503                              今のおススメはある？\n",
       "2     65  主人と別々に会員登録したいんですけど、２人のメアドがいっしょでもできますか？\n",
       "3    110         海外お土産や旅行用品を希望の日時に郵送してもらいたいのですが。\n",
       "4    397          旅行のキャンセル料が、旅行代金より高くなることはありますか？"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data as tsv files.  \n",
    "test:dev:train = 2:2:6. To check the usability of finetuning, we also prepare sampled training data (1/5 of full training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:len(df) // 10].to_csv( os.path.join(EXTRACTDIR, \"test.tsv\"), sep='\\t', index=False)\n",
    "df[len(df) // 10:len(df)*2 // 10].to_csv( os.path.join(EXTRACTDIR, \"dev.tsv\"), sep='\\t', index=False)\n",
    "df[len(df)*2 // 10:].to_csv( os.path.join(EXTRACTDIR, \"train.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "### 1/5 of full training data.\n",
    "# df[:len(df) // 5].to_csv( os.path.join(EXTRACTDIR, \"test.tsv\"), sep='\\t', index=False)\n",
    "# df[len(df) // 5:len(df)*2 // 5].to_csv( os.path.join(EXTRACTDIR, \"dev.tsv\"), sep='\\t', index=False)\n",
    "# df[len(df)*2 // 5:].sample(frac=0.2, random_state=23).to_csv( os.path.join(EXTRACTDIR, \"train.tsv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune pre-trained model\n",
    "\n",
    "It will take a lot of hours to execute the following cells on CPU environment.  \n",
    "You can also use colab to recieve the power of TPU. You need to uplode the created data onto your GCS bucket.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1zZH2GWe0U-7GjJ2w2duodFfEUptvHjcx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_PATH = '../model/model.ckpt-1000000'\n",
    "FINETUNE_OUTPUT_DIR = '../model/his_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ckpts = glob.glob(\"{}/model.ckpt*data*\".format(FINETUNE_OUTPUT_DIR))\n",
    "latest_ckpt = sorted(output_ckpts)[-1]\n",
    "PRETRAINED_MODEL_PATH = latest_ckpt.split('.data-00000-of-00001')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/his_output/model.ckpt-39000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a trained SentencePiece model.\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f874b23fb70>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../model/his_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8749da2940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "INFO:tensorflow:Writing example 0 of 34368\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "INFO:tensorflow:tokens: [CLS] ▁ リゾート フィー は 、 追加 で 支払い が必要な 費用 ですか ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 8456 3815 11 7 3083 19 15303 15480 3681 23120 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 443 (id = 443)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "INFO:tensorflow:tokens: [CLS] ▁ 国 を また いで 鉄道 に乗る ときは 税関 は どう なり ます か [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 115 18 240 8426 397 20277 6987 28311 11 1258 1700 3418 95 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 342 (id = 342)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "INFO:tensorflow:tokens: [CLS] ▁ レン タ カー の説明 は 日本語 ですか ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 854 170 410 16633 11 2481 23120 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 322 (id = 322)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "INFO:tensorflow:tokens: [CLS] ▁ 正確な 駅名 が わからない と 探 せない でしょう か ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 8362 12094 12 17619 20 9838 13035 16744 95 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 373 (id = 373)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-5\n",
      "INFO:tensorflow:tokens: [CLS] ▁ 保険 期間 を 修正 することは でき ます か ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 3760 1496 18 3479 6603 3092 3418 95 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 130 (id = 130)\n",
      "INFO:tensorflow:Writing example 10000 of 34368\n",
      "INFO:tensorflow:Writing example 20000 of 34368\n",
      "INFO:tensorflow:Writing example 30000 of 34368\n",
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 34368\n",
      "INFO:tensorflow:  Batch size = 16\n",
      "INFO:tensorflow:  Num steps = 42960\n",
      "WARNING:tensorflow:From ../src/run_classifier.py:460: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (16, 32)\n",
      "INFO:tensorflow:  name = input_mask, shape = (16, 32)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (16,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (16,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (16, 32)\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (32000, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (537, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (537,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-01-29 02:41:05.024702: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-01-29 02:41:05.116938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-01-29 02:41:05.117417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-01-29 02:41:05.117458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
      "2019-01-29 02:41:05.409292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-29 02:41:05.409346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
      "2019-01-29 02:41:05.409357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
      "2019-01-29 02:41:05.409668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Restoring parameters from ../model/his_output/model.ckpt-39000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 39000 into ../model/his_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.79002\n",
      "INFO:tensorflow:examples/sec: 44.6403\n",
      "INFO:tensorflow:global_step/sec: 3.211\n",
      "INFO:tensorflow:examples/sec: 51.376\n",
      "INFO:tensorflow:global_step/sec: 3.2097\n",
      "INFO:tensorflow:examples/sec: 51.3552\n",
      "INFO:tensorflow:global_step/sec: 3.20221\n",
      "INFO:tensorflow:examples/sec: 51.2353\n",
      "INFO:tensorflow:global_step/sec: 3.20979\n",
      "INFO:tensorflow:examples/sec: 51.3566\n",
      "INFO:tensorflow:global_step/sec: 3.19815\n",
      "INFO:tensorflow:examples/sec: 51.1704\n",
      "INFO:tensorflow:global_step/sec: 3.21057\n",
      "INFO:tensorflow:examples/sec: 51.3691\n",
      "INFO:tensorflow:global_step/sec: 3.20879\n",
      "INFO:tensorflow:examples/sec: 51.3406\n",
      "INFO:tensorflow:global_step/sec: 3.1997\n",
      "INFO:tensorflow:examples/sec: 51.1952\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into ../model/his_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.95977\n",
      "INFO:tensorflow:examples/sec: 47.3563\n",
      "INFO:tensorflow:global_step/sec: 3.20136\n",
      "INFO:tensorflow:examples/sec: 51.2218\n",
      "INFO:tensorflow:global_step/sec: 3.2098\n",
      "INFO:tensorflow:examples/sec: 51.3569\n",
      "INFO:tensorflow:global_step/sec: 3.19869\n",
      "INFO:tensorflow:examples/sec: 51.1791\n",
      "^C\n",
      "CPU times: user 7.58 s, sys: 2.2 s, total: 9.78 s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# It will take many hours on CPU environment.\n",
    "\n",
    "!python3 ../src/run_classifier.py \\\n",
    "  --task_name=his \\\n",
    "  --do_train=true \\\n",
    "  --do_eval=true \\\n",
    "  --data_dir=../data/his \\\n",
    "  --model_file=../model/wiki-ja.model \\\n",
    "  --vocab_file=../model/wiki-ja.vocab \\\n",
    "  --init_checkpoint={PRETRAINED_MODEL_PATH} \\\n",
    "  --max_seq_length=32 \\\n",
    "  --train_batch_size=16 \\\n",
    "  --learning_rate=5e-5 \\\n",
    "  --num_train_epochs=20 \\\n",
    "  --output_dir={FINETUNE_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the finetuned model\n",
    "\n",
    "Let's predict test data using the finetuned model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import tokenization_sentencepiece as tokenization\n",
    "from run_classifier import HISProcessor\n",
    "from run_classifier import model_fn_builder\n",
    "from run_classifier import file_based_input_fn_builder\n",
    "from run_classifier import file_based_convert_examples_to_features\n",
    "from utils import str_to_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../bert\")\n",
    "\n",
    "import modeling\n",
    "import optimization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "bert_config_file = tempfile.NamedTemporaryFile(mode='w+t', encoding='utf-8', suffix='.json')\n",
    "bert_config_file.write(json.dumps({k:str_to_value(v) for k,v in config['BERT-CONFIG'].items()}))\n",
    "bert_config_file.seek(0)\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ckpts = glob.glob(\"{}/model.ckpt*data*\".format(FINETUNE_OUTPUT_DIR))\n",
    "latest_ckpt = sorted(output_ckpts)[-1]\n",
    "FINETUNED_MODEL_PATH = latest_ckpt.split('.data-00000-of-00001')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/his_output/model.ckpt-40000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINETUNED_MODEL_PATH = '../model/his_output/model.ckpt-33000'\n",
    "FINETUNED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "    '''Parameters.'''\n",
    "    def __init__(self):\n",
    "        self.model_file = \"../model/wiki-ja.model\"\n",
    "        self.vocab_file = \"../model/wiki-ja.vocab\"\n",
    "        self.do_lower_case = True\n",
    "        self.use_tpu = False\n",
    "        self.output_dir = \"/dummy\"\n",
    "        self.data_dir = \"../data/his\"\n",
    "        self.max_seq_length = 64\n",
    "        self.init_checkpoint = FINETUNED_MODEL_PATH\n",
    "        self.predict_batch_size = 4\n",
    "        \n",
    "        # The following parameters are not used in predictions.\n",
    "        # Just use to create RunConfig.\n",
    "        self.master = None\n",
    "        self.save_checkpoints_steps = 1\n",
    "        self.iterations_per_loop = 1\n",
    "        self.num_tpu_cores = 1\n",
    "        self.learning_rate = 0\n",
    "        self.num_warmup_steps = 0\n",
    "        self.num_train_steps = 0\n",
    "        self.train_batch_size = 0\n",
    "        self.eval_batch_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = FLAGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = HISProcessor()\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a trained SentencePiece model.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "    model_file=FLAGS.model_file, vocab_file=FLAGS.vocab_file,\n",
    "    do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "tpu_cluster_resolver = None\n",
    "\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    master=FLAGS.master,\n",
    "    model_dir=FLAGS.output_dir,\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "        num_shards=FLAGS.num_tpu_cores,\n",
    "        per_host_input_for_training=is_per_host))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f6871d8e8c8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/dummy', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6871d8d898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1, num_shards=1, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\n",
      "WARNING:tensorflow:Setting TPUConfig.num_shards==1 is an unsupported behavior. Please fix as soon as possible (leaving num_shards as None.)\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=FLAGS.init_checkpoint,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    num_train_steps=FLAGS.num_train_steps,\n",
    "    num_warmup_steps=FLAGS.num_warmup_steps,\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,\n",
    "    eval_batch_size=FLAGS.eval_batch_size,\n",
    "    predict_batch_size=FLAGS.predict_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 4296\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-1\n",
      "INFO:tensorflow:tokens: [CLS] ▁ ソウル の 渡航 条件 は ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 4421 10 22047 1031 11 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 532 (id = 532)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-2\n",
      "INFO:tensorflow:tokens: [CLS] ▁今 のお ス ス メ は ある ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9099 4992 60 60 401 11 382 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 503 (id = 503)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-3\n",
      "INFO:tensorflow:tokens: [CLS] ▁ 主人 と 別々に 会員 登録 したい んです けど 、 2 人の メ アド が いっ しょ でも でき ます か ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 15181 20 26409 2055 1202 6423 13391 13382 7 25 306 401 5451 12 14918 5924 153 3092 3418 95 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 65 (id = 65)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-4\n",
      "INFO:tensorflow:tokens: [CLS] ▁ 海外 お 土産 や 旅行 用品 を 希望 の 日 時に 郵 送 しても らい たい の です が 。 [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 1355 220 24671 26 2640 11519 18 5072 10 33 576 27361 3143 3557 4007 1035 10 2767 12 8 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 110 (id = 110)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: test-5\n",
      "INFO:tensorflow:tokens: [CLS] ▁ 旅行 の キャンセル 料 が 、 旅行 代金 より 高 くなる ことは あります か ? [SEP]\n",
      "INFO:tensorflow:input_ids: 4 9 2640 10 19863 2508 12 7 2640 18354 94 150 7246 1450 27541 95 3017 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 397 (id = 397)\n"
     ]
    }
   ],
   "source": [
    "predict_examples = processor.get_test_examples(FLAGS.data_dir)\n",
    "predict_file = tempfile.NamedTemporaryFile(mode='w+t', encoding='utf-8', suffix='.tf_record')\n",
    "\n",
    "file_based_convert_examples_to_features(predict_examples, label_list,\n",
    "                                        FLAGS.max_seq_length, tokenizer,\n",
    "                                        predict_file.name)\n",
    "\n",
    "predict_drop_remainder = True if FLAGS.use_tpu else False\n",
    "\n",
    "predict_input_fn = file_based_input_fn_builder(\n",
    "    input_file=predict_file.name,\n",
    "    seq_length=FLAGS.max_seq_length,\n",
    "    is_training=False,\n",
    "    drop_remainder=predict_drop_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /dummy, running initialization to predict.\n",
      "WARNING:tensorflow:From ../src/run_classifier.py:460: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 64)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 64)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 64)\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (32000, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (537, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (537,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "CPU times: user 38.4 s, sys: 13.2 s, total: 51.6 s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# It will take a few hours on CPU environment.\n",
    "\n",
    "result = list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'probabilities': array([4.85639612e-04, 6.14706893e-04, 4.16122144e-04, 1.23017665e-03,\n",
       "         2.44979965e-05, 2.90688733e-03, 4.99869976e-03, 1.45811234e-02,\n",
       "         5.02463547e-04, 4.87088400e-04, 1.95992776e-04, 1.76234171e-04,\n",
       "         1.42338577e-05, 6.28136331e-04, 4.00650053e-04, 1.15163450e-03,\n",
       "         4.06570500e-04, 3.39805876e-04, 5.63080609e-03, 1.51450816e-03,\n",
       "         2.91109900e-04, 1.97184039e-04, 1.29429638e-04, 3.00003216e-03,\n",
       "         2.42308085e-03, 1.56356327e-04, 6.43298030e-04, 2.13932639e-04,\n",
       "         6.10648131e-05, 3.71956194e-05, 1.84467397e-04, 1.99013331e-04,\n",
       "         6.56422751e-04, 1.35776791e-05, 2.00152892e-04, 4.97852561e-05,\n",
       "         7.68852042e-05, 1.43743091e-05, 1.03230227e-03, 4.66304744e-04,\n",
       "         7.00645789e-04, 1.72082437e-05, 8.09362798e-04, 2.83179659e-04,\n",
       "         3.55601187e-05, 4.40805416e-05, 5.46655501e-05, 1.46876162e-04,\n",
       "         1.28057401e-03, 9.74849245e-05, 8.07984639e-03, 4.51951623e-02,\n",
       "         1.86495134e-03, 7.51424523e-04, 1.03893422e-03, 2.30080634e-03,\n",
       "         2.27799974e-04, 1.90915729e-04, 2.61236710e-04, 7.14127600e-05,\n",
       "         1.07377663e-03, 3.78888054e-03, 7.88636040e-04, 1.26740022e-03,\n",
       "         5.76307757e-05, 3.58624937e-04, 5.56616542e-05, 1.03132357e-03,\n",
       "         5.80107386e-04, 1.45702856e-02, 8.36685300e-04, 8.90103914e-03,\n",
       "         5.09867817e-02, 3.14253196e-02, 1.40902994e-04, 5.41764195e-04,\n",
       "         2.25137308e-04, 4.93091007e-04, 1.06928591e-03, 4.59878152e-04,\n",
       "         2.09135498e-04, 3.66469234e-04, 2.87327170e-03, 2.23828843e-04,\n",
       "         2.04262068e-03, 8.38488166e-04, 3.62972933e-04, 2.94209312e-05,\n",
       "         4.19698481e-04, 1.10851449e-03, 2.15938257e-04, 1.95265631e-04,\n",
       "         3.60253704e-04, 7.00760138e-05, 4.56158996e-05, 4.62376687e-04,\n",
       "         7.41876938e-05, 1.24329017e-04, 4.43754579e-05, 2.30079677e-05,\n",
       "         2.41627160e-04, 1.92255073e-04, 1.84185876e-04, 4.92339663e-04,\n",
       "         8.17926048e-06, 7.02148245e-05, 4.21080658e-05, 1.35261042e-04,\n",
       "         3.34377721e-04, 4.98478184e-04, 6.01614767e-04, 1.89810223e-03,\n",
       "         4.55559290e-04, 1.01157369e-04, 1.18036995e-04, 1.06800138e-03,\n",
       "         1.32495174e-04, 6.37611316e-04, 1.50427571e-04, 2.28705729e-04,\n",
       "         2.65003357e-04, 1.08863424e-04, 7.64460347e-05, 2.36987346e-03,\n",
       "         4.63761826e-04, 1.11043861e-03, 7.81196635e-03, 6.90515852e-04,\n",
       "         1.85771915e-03, 5.60122961e-03, 4.39446361e-04, 1.82458127e-04,\n",
       "         8.75740516e-05, 7.76505549e-05, 7.16904935e-04, 1.18649499e-04,\n",
       "         9.95257433e-05, 5.58605971e-05, 6.40783692e-04, 7.17469433e-04,\n",
       "         1.20147044e-04, 5.22975926e-04, 5.66934352e-04, 7.76906731e-04,\n",
       "         1.43435755e-04, 1.28135449e-04, 5.22205897e-04, 3.00843647e-04,\n",
       "         2.03508433e-04, 3.87363741e-03, 1.61502481e-04, 2.04278811e-04,\n",
       "         3.48872226e-03, 6.33106241e-03, 1.12822466e-03, 1.09186885e-03,\n",
       "         2.06134529e-04, 1.73937719e-04, 2.25008640e-04, 3.74366355e-04,\n",
       "         1.64494291e-03, 3.26973619e-04, 7.68389145e-04, 2.46102223e-04,\n",
       "         2.43138900e-04, 1.40824806e-04, 1.54622074e-03, 5.17246990e-05,\n",
       "         1.44148082e-03, 3.84372863e-04, 1.16055999e-04, 1.43091043e-03,\n",
       "         2.81303469e-02, 1.55765691e-03, 9.30195209e-04, 1.07479142e-02,\n",
       "         6.61157479e-04, 4.14417242e-04, 6.90427981e-03, 2.38365465e-04,\n",
       "         1.90074134e-05, 4.49218322e-03, 1.08468253e-02, 1.79532624e-03,\n",
       "         3.14618601e-03, 1.85265113e-03, 3.63053230e-04, 8.99212551e-04,\n",
       "         2.84859678e-04, 2.55983789e-03, 1.36883487e-03, 5.30525322e-05,\n",
       "         1.42245161e-04, 1.68721381e-05, 3.83784412e-04, 7.82836469e-06,\n",
       "         4.37536743e-04, 5.25285257e-04, 4.76490004e-06, 7.40838004e-04,\n",
       "         5.33735583e-05, 1.66210136e-03, 1.31812281e-04, 2.42054521e-04,\n",
       "         1.54022622e-04, 8.81566739e-05, 1.04377512e-04, 1.00597508e-05,\n",
       "         1.47035797e-04, 2.73893202e-05, 1.01370264e-04, 1.78556686e-04,\n",
       "         4.82939431e-05, 2.64926814e-04, 4.99893540e-05, 5.55120641e-04,\n",
       "         8.49263506e-06, 2.53144608e-05, 2.50705314e-04, 2.46627576e-04,\n",
       "         5.46515294e-05, 4.15430230e-04, 1.07742111e-04, 9.81395788e-05,\n",
       "         2.72548670e-04, 8.88996292e-05, 1.11720074e-04, 1.53197645e-04,\n",
       "         3.21360363e-04, 6.84953775e-05, 3.79478530e-04, 6.71480782e-04,\n",
       "         7.82873947e-04, 8.87878239e-04, 1.19289965e-03, 8.01626500e-03,\n",
       "         3.37451311e-05, 4.68200073e-03, 2.52265087e-03, 2.40923517e-04,\n",
       "         1.90643361e-04, 5.84126916e-04, 1.54202594e-03, 2.58354424e-03,\n",
       "         2.61413655e-03, 7.36150323e-05, 4.90260730e-03, 1.08559296e-04,\n",
       "         3.91151843e-05, 8.22844449e-05, 4.04590042e-03, 2.92925467e-03,\n",
       "         2.20209651e-04, 1.01601865e-04, 9.47523222e-04, 9.07765498e-05,\n",
       "         1.71138599e-05, 1.51566626e-03, 1.73134304e-05, 1.07192363e-05,\n",
       "         6.01769389e-05, 8.20527712e-05, 5.74152946e-05, 1.34225527e-04,\n",
       "         2.44227413e-04, 2.62699352e-04, 1.38418436e-05, 1.19780612e-04,\n",
       "         1.00859659e-04, 4.14264097e-04, 7.37469585e-04, 1.58776704e-03,\n",
       "         1.62076985e-03, 3.03986977e-04, 3.47458845e-04, 5.06634125e-04,\n",
       "         1.50763232e-03, 2.60660105e-04, 1.32046436e-04, 8.56283295e-04,\n",
       "         4.83711512e-04, 9.53928793e-06, 1.26187471e-04, 2.15647873e-04,\n",
       "         7.48686725e-04, 7.72442127e-06, 5.91873359e-05, 8.11170554e-04,\n",
       "         7.00066448e-04, 6.55056676e-04, 8.59726497e-05, 3.05688387e-04,\n",
       "         1.71591295e-04, 5.58071624e-05, 1.02231163e-04, 1.83539101e-04,\n",
       "         4.70181694e-04, 4.34968708e-04, 1.24513952e-03, 8.41221336e-05,\n",
       "         3.12779121e-05, 2.94112804e-04, 1.83427299e-04, 2.21128226e-04,\n",
       "         6.80658675e-04, 1.33852256e-04, 1.66223047e-03, 4.33581481e-05,\n",
       "         2.36617416e-05, 9.09491326e-04, 2.37934164e-05, 9.97973330e-05,\n",
       "         4.23681995e-05, 1.29695050e-03, 3.90195608e-04, 1.79372169e-03,\n",
       "         2.14425940e-03, 4.28884476e-03, 5.02809417e-04, 3.52311158e-03,\n",
       "         5.42337249e-04, 2.16790242e-03, 1.48866698e-02, 1.32097560e-03,\n",
       "         3.28287482e-04, 2.49787383e-02, 1.44245022e-03, 1.97279290e-03,\n",
       "         2.43532006e-03, 1.60979282e-03, 1.23194361e-03, 2.44813273e-03,\n",
       "         7.16995855e-05, 1.31060695e-03, 2.33743014e-03, 9.43867839e-04,\n",
       "         7.39545503e-04, 4.33319219e-04, 4.53867833e-05, 2.71492987e-03,\n",
       "         1.58642617e-03, 9.57636908e-03, 9.99115184e-02, 7.96276072e-05,\n",
       "         1.38462929e-04, 9.15212149e-04, 5.83962072e-03, 8.29208002e-04,\n",
       "         2.58987932e-03, 1.28905859e-03, 8.36223015e-04, 7.94505700e-04,\n",
       "         3.99732031e-03, 1.45075871e-02, 4.35985799e-04, 1.63405595e-04,\n",
       "         1.12309656e-03, 2.47069565e-03, 4.87478566e-04, 5.50836034e-04,\n",
       "         3.42232035e-03, 1.73826213e-03, 1.03208609e-03, 8.51671270e-04,\n",
       "         1.86472360e-04, 5.77704923e-04, 1.10844220e-03, 2.89989093e-05,\n",
       "         9.16352728e-04, 1.93690445e-04, 2.65710551e-05, 4.53195717e-05,\n",
       "         2.35450425e-04, 3.43765336e-04, 2.61466252e-04, 3.66266395e-05,\n",
       "         1.80600735e-04, 4.81068055e-05, 5.50050230e-04, 1.45081605e-04,\n",
       "         1.22427446e-04, 2.30628884e-05, 6.60339720e-05, 4.05210245e-04,\n",
       "         1.41561951e-03, 1.21484438e-04, 7.54115172e-05, 1.47724641e-04,\n",
       "         1.04560073e-04, 4.26714141e-05, 1.20301549e-04, 1.09589986e-04,\n",
       "         1.36645496e-04, 2.38315799e-04, 1.02005950e-04, 3.38496116e-04,\n",
       "         3.12638556e-04, 1.77585986e-04, 6.43117382e-05, 1.74372268e-04,\n",
       "         1.25074424e-04, 2.35905428e-03, 5.06009499e-04, 2.40929818e-04,\n",
       "         1.03751678e-04, 4.35663242e-04, 2.58680229e-04, 1.38127478e-04,\n",
       "         1.01141643e-03, 5.02920419e-04, 1.73006643e-04, 1.74124943e-04,\n",
       "         1.25167382e-04, 1.61510397e-04, 5.18634508e-04, 1.01601193e-03,\n",
       "         1.75419977e-04, 5.08961792e-04, 1.40663933e-05, 3.73363546e-05,\n",
       "         4.80417744e-04, 1.66161612e-04, 2.72145728e-04, 1.17142983e-02,\n",
       "         9.81775156e-06, 1.41702301e-03, 4.57795511e-04, 8.39419838e-04,\n",
       "         3.82252445e-04, 4.22025420e-04, 3.36213794e-04, 8.76561026e-05,\n",
       "         2.18996211e-04, 1.13683590e-03, 8.99175720e-05, 3.42325657e-05,\n",
       "         4.64550831e-05, 3.11055919e-04, 7.65923542e-05, 4.98389309e-05,\n",
       "         3.81821743e-03, 5.43123693e-04, 4.89654904e-03, 9.83319478e-04,\n",
       "         1.62639742e-04, 2.42375466e-03, 8.63828755e-05, 9.07102250e-04,\n",
       "         1.96605877e-04, 8.63687455e-05, 6.92209403e-04, 7.96302702e-05,\n",
       "         4.72932588e-04, 1.13124494e-02, 4.62314725e-04, 3.35014309e-04,\n",
       "         8.69396681e-05, 2.02183277e-04, 1.05224040e-04, 1.08677591e-03,\n",
       "         9.22960404e-04, 4.20124410e-03, 1.01022841e-03, 5.14867716e-04,\n",
       "         3.69703812e-05, 5.97768836e-03, 1.16977002e-03, 1.00104406e-03,\n",
       "         3.05431517e-04, 1.34212154e-04, 7.35538080e-04, 4.78317743e-05,\n",
       "         3.37286087e-06, 6.28866081e-04, 1.41457494e-04, 3.79097124e-04,\n",
       "         9.78006865e-04, 4.38073446e-04, 2.43223663e-02, 3.54104093e-03,\n",
       "         7.61126517e-04, 8.98839266e-04, 9.20879247e-05, 1.57402852e-03,\n",
       "         6.26095533e-02, 3.56002798e-04, 4.23783422e-05, 2.22658506e-04,\n",
       "         1.64611425e-04, 5.61677851e-04, 4.23827092e-04, 1.38454349e-03,\n",
       "         4.60020703e-04, 1.77087844e-04, 2.87626346e-04, 3.92744765e-02,\n",
       "         5.20420610e-04, 1.28251372e-03, 8.62072557e-05, 2.33482584e-04,\n",
       "         3.29960138e-03, 2.10625585e-04, 1.85071607e-04, 1.56500586e-03,\n",
       "         1.66291138e-04, 3.20070027e-03, 7.12350593e-05, 3.17966542e-03,\n",
       "         5.93325007e-04, 4.39191412e-04, 3.24122637e-04, 3.75571661e-04,\n",
       "         1.49199448e-04, 6.11258656e-05, 1.59320858e-04, 2.63288128e-03,\n",
       "         5.86760975e-03, 8.32271962e-06, 2.28929493e-05, 2.16310276e-04,\n",
       "         1.02556281e-04, 4.12031281e-04, 1.50136053e-04, 2.90445896e-04,\n",
       "         7.72448769e-03, 5.65365015e-04, 1.47353252e-03, 4.14564833e-03,\n",
       "         5.84294030e-04, 9.54304705e-04, 3.51034920e-04, 1.72348737e-04,\n",
       "         3.29480283e-02, 1.90940965e-03, 3.28995484e-05, 2.19131913e-02,\n",
       "         1.49142536e-04], dtype=float32)},\n",
       " {'probabilities': array([4.74328757e-04, 1.61735370e-04, 1.17805568e-04, 4.35497565e-03,\n",
       "         1.07434516e-05, 1.83873217e-05, 2.51549081e-05, 1.12144342e-04,\n",
       "         1.40604652e-05, 4.96684988e-05, 7.78825142e-06, 1.38020878e-05,\n",
       "         6.96853158e-06, 1.02614695e-05, 5.56638861e-06, 8.24577510e-05,\n",
       "         6.15964818e-05, 4.54889196e-05, 3.58496560e-04, 7.20600874e-05,\n",
       "         1.09088549e-04, 1.50744245e-05, 1.78627470e-05, 4.13799171e-05,\n",
       "         2.29924932e-04, 5.64805669e-05, 7.58260838e-04, 2.26186912e-05,\n",
       "         7.50257004e-06, 1.69619543e-05, 9.41846520e-06, 1.44278329e-05,\n",
       "         1.28518470e-04, 1.88160482e-06, 3.03739380e-05, 5.09046458e-06,\n",
       "         3.94875169e-05, 1.82567965e-05, 2.59515073e-04, 5.79166554e-05,\n",
       "         2.11929419e-05, 7.27552060e-06, 1.82872209e-05, 6.00367002e-06,\n",
       "         1.27231272e-06, 3.27203657e-06, 5.30160651e-06, 3.62093197e-05,\n",
       "         8.67505441e-05, 5.41215340e-05, 4.52474365e-03, 3.98159755e-04,\n",
       "         4.93217522e-05, 2.47926127e-05, 7.53363347e-05, 4.06953245e-01,\n",
       "         3.27280322e-05, 6.68212306e-05, 5.57416861e-05, 7.62279378e-05,\n",
       "         1.27831256e-04, 1.20917910e-04, 3.26392910e-05, 4.80505521e-04,\n",
       "         2.02083629e-06, 1.57269169e-04, 6.48229707e-06, 1.09187677e-05,\n",
       "         1.05197905e-05, 4.03561062e-05, 1.86178906e-04, 4.36272589e-04,\n",
       "         1.99965155e-03, 7.73301697e-04, 7.07507797e-06, 6.78258875e-05,\n",
       "         1.84426855e-04, 3.65406208e-06, 1.17269759e-04, 2.64637802e-05,\n",
       "         1.91566302e-04, 1.50946289e-04, 1.02989747e-04, 2.73614660e-05,\n",
       "         9.99261392e-05, 1.80439165e-04, 3.71861170e-05, 1.74277357e-05,\n",
       "         2.53529015e-05, 5.73876277e-05, 4.73842047e-05, 2.96430335e-05,\n",
       "         6.91220521e-06, 1.80853076e-05, 5.37092674e-05, 9.94388847e-06,\n",
       "         6.87136389e-06, 9.59925546e-06, 5.61083834e-06, 1.41706278e-06,\n",
       "         5.39772827e-06, 6.69686324e-06, 1.11224421e-04, 4.92754443e-05,\n",
       "         4.81628786e-06, 6.65559526e-07, 8.01949227e-06, 1.26345344e-06,\n",
       "         1.44989579e-04, 4.41483207e-05, 2.27356039e-04, 4.22209014e-05,\n",
       "         3.26961163e-04, 5.75643462e-05, 3.10948417e-06, 4.72918400e-05,\n",
       "         1.34927559e-05, 2.82740104e-04, 3.68588553e-05, 1.88923677e-05,\n",
       "         1.26606183e-05, 4.41955053e-05, 1.35582959e-05, 1.06827691e-04,\n",
       "         3.27518828e-05, 6.37902704e-05, 4.36858572e-05, 1.40069978e-05,\n",
       "         1.82379154e-05, 3.39294274e-05, 6.58945464e-06, 1.49644475e-05,\n",
       "         2.34706440e-05, 6.08382870e-05, 1.18121749e-03, 3.68677829e-05,\n",
       "         5.16406653e-05, 2.32989023e-05, 8.27272088e-05, 1.66257363e-04,\n",
       "         3.27327725e-05, 2.08430956e-05, 1.46574266e-05, 1.56845072e-05,\n",
       "         1.86598227e-05, 6.05764333e-03, 7.03209080e-05, 5.43897113e-05,\n",
       "         5.70214579e-05, 2.40762194e-04, 5.05510798e-05, 1.20661998e-05,\n",
       "         5.69812022e-04, 3.23182147e-04, 1.54037843e-04, 2.47678137e-04,\n",
       "         1.07479709e-05, 1.70339437e-04, 1.42442688e-04, 8.14557425e-05,\n",
       "         5.37637934e-05, 1.21746216e-05, 4.83214026e-05, 2.39869823e-05,\n",
       "         8.09087305e-06, 6.04193265e-05, 3.38621248e-05, 2.23100851e-05,\n",
       "         1.31253270e-04, 7.12476030e-05, 5.58318643e-05, 9.66286578e-04,\n",
       "         5.69174183e-04, 5.67662282e-05, 3.40254992e-05, 8.34060265e-05,\n",
       "         2.85607995e-04, 1.10821020e-04, 3.22471443e-03, 1.19325296e-05,\n",
       "         2.23818188e-05, 2.88516487e-04, 4.08586289e-04, 8.10287602e-05,\n",
       "         9.53378258e-05, 9.32193361e-05, 1.86342695e-05, 4.53798602e-05,\n",
       "         3.81041755e-05, 6.75124465e-05, 1.07689171e-04, 6.99687007e-05,\n",
       "         4.40622243e-05, 2.80263953e-06, 8.95135599e-06, 1.97775285e-06,\n",
       "         3.47518617e-05, 5.86185852e-05, 6.18135005e-07, 6.12344593e-05,\n",
       "         4.89512513e-06, 2.06684927e-05, 6.29455681e-05, 2.47626504e-05,\n",
       "         3.80456040e-05, 7.60713237e-06, 7.26540893e-05, 1.90611718e-06,\n",
       "         4.99760063e-05, 9.64903847e-06, 5.22503069e-06, 1.43481511e-05,\n",
       "         3.12544012e-06, 2.18274254e-05, 6.91799551e-06, 3.42782696e-05,\n",
       "         2.91207243e-06, 2.82996643e-05, 2.03608870e-05, 1.78263290e-05,\n",
       "         7.52045380e-05, 5.19845271e-05, 1.62821481e-04, 5.58198008e-06,\n",
       "         4.89452541e-05, 4.53735884e-05, 7.82968418e-05, 6.58689460e-05,\n",
       "         1.14781004e-04, 7.43962846e-06, 1.33265930e-04, 4.76289460e-05,\n",
       "         1.92268471e-05, 2.11429986e-04, 1.63669960e-04, 6.87088957e-03,\n",
       "         2.67388041e-05, 2.94956644e-05, 2.78966239e-04, 2.75647835e-05,\n",
       "         1.13617580e-05, 4.16873663e-05, 1.19964214e-04, 8.47324045e-05,\n",
       "         1.48146893e-04, 4.16779267e-06, 9.07070236e-04, 4.77258400e-05,\n",
       "         8.60360979e-06, 1.20612522e-05, 2.61889800e-04, 2.02108218e-04,\n",
       "         6.85312843e-05, 1.03707025e-05, 3.89668494e-05, 1.36023300e-05,\n",
       "         1.03255616e-05, 4.17931093e-04, 2.12753403e-05, 3.87632736e-06,\n",
       "         1.51075176e-06, 2.41998816e-04, 4.68936332e-06, 1.23814796e-04,\n",
       "         5.39967696e-05, 1.27545456e-04, 3.91060667e-06, 1.84912133e-05,\n",
       "         3.28816059e-05, 2.59728336e-06, 8.74807756e-06, 1.43739759e-04,\n",
       "         9.09679075e-05, 5.51149242e-05, 2.59228345e-05, 2.71078152e-05,\n",
       "         1.98186808e-05, 4.44663710e-05, 2.59590051e-05, 3.06320217e-05,\n",
       "         1.09950430e-04, 1.66016378e-06, 1.82025560e-05, 3.07199043e-05,\n",
       "         3.65164829e-04, 4.20976812e-06, 1.10017490e-05, 3.48527760e-06,\n",
       "         3.24966059e-05, 1.00689322e-04, 9.47642457e-06, 7.25802238e-06,\n",
       "         4.32030574e-05, 2.61112564e-05, 2.80771856e-05, 6.34554090e-05,\n",
       "         5.77926221e-05, 1.21146091e-04, 6.57710189e-05, 3.81193313e-05,\n",
       "         1.11011259e-05, 6.25680026e-04, 5.68750911e-05, 7.12444089e-05,\n",
       "         2.93714925e-06, 4.23746278e-05, 3.40662984e-04, 2.03382006e-05,\n",
       "         3.39813669e-05, 7.60129842e-05, 3.99769560e-05, 1.62363522e-05,\n",
       "         6.75625824e-06, 3.15877260e-05, 1.66803522e-06, 4.24976970e-05,\n",
       "         4.73949149e-05, 1.15384413e-04, 3.77356337e-05, 1.22441648e-04,\n",
       "         7.42048578e-05, 2.44659659e-05, 3.65428015e-04, 4.77824069e-05,\n",
       "         7.23347475e-05, 5.05928765e-04, 5.46055053e-05, 2.14143030e-04,\n",
       "         1.73802415e-04, 2.67874766e-05, 3.98755947e-05, 2.10336129e-05,\n",
       "         1.95034954e-05, 1.32586458e-04, 7.28164974e-04, 1.67139835e-04,\n",
       "         4.53218636e-05, 8.53381935e-05, 1.17635755e-05, 5.29048557e-04,\n",
       "         7.04180129e-05, 5.04886673e-04, 2.14583779e-04, 2.98635200e-06,\n",
       "         1.14149761e-05, 1.67299630e-04, 3.65529268e-04, 5.76669932e-04,\n",
       "         1.40866789e-03, 4.37286886e-04, 1.00834623e-04, 5.95317862e-04,\n",
       "         4.54320427e-04, 1.57282513e-04, 2.61844925e-05, 3.62152764e-06,\n",
       "         2.45163901e-05, 4.37162853e-05, 2.26243410e-05, 6.04667657e-05,\n",
       "         1.48916777e-04, 4.20169672e-05, 4.51131054e-05, 6.35762481e-05,\n",
       "         1.60144773e-05, 2.06819805e-05, 5.18386260e-05, 8.78585979e-06,\n",
       "         1.24133239e-05, 6.84794140e-05, 5.05708749e-06, 1.35217660e-05,\n",
       "         8.53349775e-06, 2.71996141e-05, 1.71117081e-05, 2.33515198e-06,\n",
       "         1.25418860e-03, 2.08541696e-05, 5.46228490e-04, 2.65046729e-05,\n",
       "         1.20060631e-05, 2.61261739e-05, 2.44160365e-05, 6.69435349e-06,\n",
       "         5.53656864e-05, 1.61661353e-06, 6.35766446e-06, 1.91992513e-05,\n",
       "         3.00567972e-05, 8.13971565e-06, 3.24393186e-05, 3.21691077e-05,\n",
       "         2.00347095e-05, 1.97995250e-05, 2.99276526e-05, 1.58287166e-05,\n",
       "         1.86870253e-04, 4.79733499e-05, 1.03471153e-04, 1.07455417e-05,\n",
       "         4.04623688e-06, 2.64405717e-05, 4.13268353e-05, 1.01709375e-05,\n",
       "         7.60288185e-06, 1.58878422e-04, 1.04211904e-04, 5.02868352e-06,\n",
       "         7.43985947e-05, 6.35224933e-05, 5.68995601e-05, 5.12531624e-05,\n",
       "         8.88603427e-06, 4.85036835e-05, 3.69704903e-05, 2.57952870e-05,\n",
       "         8.75146143e-05, 5.04064665e-05, 8.69563337e-06, 1.67501512e-05,\n",
       "         1.07978294e-05, 1.82886524e-05, 3.26904737e-05, 8.03676448e-06,\n",
       "         1.41347464e-05, 5.05883618e-05, 1.73300850e-05, 6.45228865e-05,\n",
       "         4.66851852e-05, 1.14218155e-05, 6.76767959e-04, 8.31688976e-06,\n",
       "         1.13048754e-05, 1.49902509e-04, 1.28058618e-05, 6.88051159e-06,\n",
       "         3.62243773e-05, 2.98452414e-06, 3.05951762e-05, 2.04697517e-06,\n",
       "         2.92897777e-04, 1.96961846e-05, 4.28632135e-04, 1.70496773e-04,\n",
       "         4.69663610e-06, 1.96128429e-04, 3.05362337e-05, 4.28110161e-06,\n",
       "         3.41156992e-05, 2.59218564e-06, 1.11027772e-03, 3.56061037e-06,\n",
       "         7.58682145e-05, 2.34244129e-04, 3.09007097e-04, 7.13412555e-06,\n",
       "         3.35910481e-05, 4.09893517e-04, 3.57089230e-05, 1.22541285e-04,\n",
       "         3.89807137e-05, 2.01681440e-04, 3.48609756e-05, 1.23947699e-04,\n",
       "         7.82642092e-05, 3.71222668e-05, 6.48809282e-06, 9.25942004e-06,\n",
       "         1.07744636e-04, 6.38934989e-06, 4.37075534e-04, 7.72903149e-05,\n",
       "         2.07807534e-05, 8.35829906e-06, 3.44386353e-05, 1.78158753e-05,\n",
       "         1.65694428e-03, 2.00040013e-04, 1.03158818e-04, 3.91752910e-05,\n",
       "         7.22677942e-05, 3.17125785e-04, 4.07652533e-06, 1.29624877e-05,\n",
       "         7.39818951e-03, 7.11448010e-05, 2.15455148e-05, 2.56936328e-04,\n",
       "         9.08702259e-06, 1.47925544e-04, 2.46751093e-04, 1.56941795e-04,\n",
       "         1.35488037e-04, 1.92174248e-05, 5.69442345e-05, 1.05407997e-03,\n",
       "         2.54031707e-04, 4.86291974e-05, 7.84237636e-06, 2.72977661e-04,\n",
       "         7.73095599e-05, 1.11644667e-05, 5.63418507e-06, 5.02861023e-01,\n",
       "         4.78290895e-05, 4.93227890e-05, 1.44486521e-05, 1.79943145e-05,\n",
       "         7.80310875e-05, 7.36613874e-05, 6.62820094e-05, 5.75376143e-05,\n",
       "         4.64550612e-06, 9.01566273e-06, 6.99533211e-06, 1.05195609e-03,\n",
       "         2.75737111e-04, 6.81765050e-06, 2.77999902e-06, 4.13435082e-05,\n",
       "         9.30721508e-05, 7.53240056e-06, 6.52277959e-04, 1.29289801e-05,\n",
       "         1.56774695e-04, 5.71571873e-05, 9.83386562e-05, 2.61040026e-04,\n",
       "         1.09505118e-03, 1.29317239e-04, 1.46993465e-04, 8.00098314e-06,\n",
       "         1.16564632e-04, 2.76982792e-05, 2.06346769e-04, 2.86520051e-04,\n",
       "         3.16951810e-05], dtype=float32)}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data set and add prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/his/test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predict'] = [ label_list[elem['probabilities'].argmax()] for elem in result ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532</td>\n",
       "      <td>ソウルの渡航条件は？</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>今のおススメはある？</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>主人と別々に会員登録したいんですけど、２人のメアドがいっしょでもできますか？</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>海外お土産や旅行用品を希望の日時に郵送してもらいたいのですが。</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>397</td>\n",
       "      <td>旅行のキャンセル料が、旅行代金より高くなることはありますか？</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>301</td>\n",
       "      <td>当日のタイムスケジュールが知りたい。</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>313</td>\n",
       "      <td>レンタカーのキャンセル料はいつからかかりますか</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>インターネットでカード番号を入れるのは安全なんですか？</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86</td>\n",
       "      <td>webサイトの会員の退会方法が知りたいです。</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>250</td>\n",
       "      <td>スケジュールが変更になると、eチケットも新しくなるんですか？</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                    text  predict\n",
       "0    532                              ソウルの渡航条件は？      342\n",
       "1    503                              今のおススメはある？      503\n",
       "2     65  主人と別々に会員登録したいんですけど、２人のメアドがいっしょでもできますか？       65\n",
       "3    110         海外お土産や旅行用品を希望の日時に郵送してもらいたいのですが。      110\n",
       "4    397          旅行のキャンセル料が、旅行代金より高くなることはありますか？      397\n",
       "5    301                      当日のタイムスケジュールが知りたい。      301\n",
       "6    313                 レンタカーのキャンセル料はいつからかかりますか      312\n",
       "7     32             インターネットでカード番号を入れるのは安全なんですか？       32\n",
       "8     86                  webサイトの会員の退会方法が知りたいです。       86\n",
       "9    250          スケジュールが変更になると、eチケットも新しくなるんですか？      250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845903165735568"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( test_df['label'] == test_df['predict'] ) / len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A littel more detailed check using `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ubuntu/anaconda3/lib/python3.6/site-packages (0.19.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         8\n",
      "          1       1.00      0.89      0.94         9\n",
      "          2       0.57      0.80      0.67         5\n",
      "          3       1.00      0.50      0.67         2\n",
      "          4       1.00      0.64      0.78        11\n",
      "          5       1.00      1.00      1.00         5\n",
      "          6       0.60      0.75      0.67         8\n",
      "          7       0.71      0.71      0.71         7\n",
      "          8       0.88      0.70      0.78        10\n",
      "          9       1.00      1.00      1.00         6\n",
      "         10       0.89      0.73      0.80        11\n",
      "         11       0.67      0.67      0.67         6\n",
      "         12       0.80      0.73      0.76        11\n",
      "         13       0.75      0.82      0.78        11\n",
      "         14       0.88      0.88      0.88         8\n",
      "         15       0.75      0.60      0.67         5\n",
      "         16       1.00      0.62      0.77         8\n",
      "         17       0.29      0.71      0.42         7\n",
      "         18       1.00      0.57      0.73         7\n",
      "         19       0.67      0.86      0.75         7\n",
      "         20       0.80      0.80      0.80        10\n",
      "         21       1.00      0.71      0.83         7\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       0.50      0.45      0.48        11\n",
      "         24       0.67      1.00      0.80         4\n",
      "         25       0.91      1.00      0.95        10\n",
      "         26       0.83      0.83      0.83         6\n",
      "         27       0.60      1.00      0.75         9\n",
      "         28       0.83      0.45      0.59        11\n",
      "         29       0.89      0.89      0.89         9\n",
      "         30       0.64      0.78      0.70         9\n",
      "         31       0.78      0.88      0.82         8\n",
      "         32       0.86      0.86      0.86         7\n",
      "         33       0.50      0.71      0.59         7\n",
      "         34       0.67      0.80      0.73        10\n",
      "         35       1.00      1.00      1.00        12\n",
      "         36       0.80      0.89      0.84         9\n",
      "         37       0.50      0.25      0.33         4\n",
      "         38       1.00      0.89      0.94         9\n",
      "         39       1.00      0.67      0.80         6\n",
      "         40       1.00      0.75      0.86         8\n",
      "         41       0.62      0.62      0.62         8\n",
      "         42       0.33      0.50      0.40         6\n",
      "         43       0.86      0.86      0.86         7\n",
      "         44       0.88      0.78      0.82         9\n",
      "         45       1.00      1.00      1.00         6\n",
      "         46       0.88      0.58      0.70        12\n",
      "         47       0.60      0.38      0.46         8\n",
      "         48       0.89      0.89      0.89         9\n",
      "         49       0.88      1.00      0.93         7\n",
      "         50       0.42      0.71      0.53         7\n",
      "         51       0.70      0.78      0.74         9\n",
      "         52       0.83      1.00      0.91         5\n",
      "         53       0.62      0.83      0.71         6\n",
      "         54       0.38      0.50      0.43         6\n",
      "         55       0.85      0.92      0.88        12\n",
      "         56       0.38      0.75      0.50         8\n",
      "         57       1.00      0.86      0.92         7\n",
      "         58       1.00      0.89      0.94         9\n",
      "         59       0.43      1.00      0.60         3\n",
      "         60       0.92      1.00      0.96        11\n",
      "         61       0.80      1.00      0.89         4\n",
      "         62       1.00      0.86      0.92        14\n",
      "         63       0.86      1.00      0.92         6\n",
      "         64       1.00      0.14      0.25         7\n",
      "         65       0.90      1.00      0.95         9\n",
      "         66       0.50      1.00      0.67         6\n",
      "         67       0.83      0.83      0.83        12\n",
      "         68       0.50      0.50      0.50         4\n",
      "         69       1.00      0.83      0.91         6\n",
      "         70       0.75      1.00      0.86         6\n",
      "         71       0.90      0.90      0.90        10\n",
      "         72       1.00      1.00      1.00        10\n",
      "         73       0.89      0.89      0.89         9\n",
      "         74       0.67      0.75      0.71         8\n",
      "         75       1.00      0.69      0.82        13\n",
      "         76       0.83      0.62      0.71         8\n",
      "         77       1.00      0.90      0.95        10\n",
      "         78       0.83      0.83      0.83         6\n",
      "         79       0.62      1.00      0.77         5\n",
      "         80       0.58      0.88      0.70         8\n",
      "         81       1.00      0.50      0.67         8\n",
      "         82       1.00      0.67      0.80         3\n",
      "         83       1.00      1.00      1.00        11\n",
      "         84       0.80      1.00      0.89         4\n",
      "         85       1.00      1.00      1.00         9\n",
      "         86       1.00      1.00      1.00        11\n",
      "         87       0.77      1.00      0.87        10\n",
      "         88       0.88      1.00      0.93         7\n",
      "         89       1.00      0.90      0.95        10\n",
      "         90       1.00      1.00      1.00         7\n",
      "         91       0.86      0.86      0.86         7\n",
      "         92       0.83      1.00      0.91         5\n",
      "         93       0.89      0.73      0.80        11\n",
      "         94       0.62      0.62      0.62         8\n",
      "         95       1.00      0.89      0.94         9\n",
      "         96       1.00      0.77      0.87        13\n",
      "         97       0.82      1.00      0.90         9\n",
      "         98       0.91      1.00      0.95        10\n",
      "         99       1.00      0.80      0.89         5\n",
      "        100       0.50      0.80      0.62         5\n",
      "        101       1.00      1.00      1.00        10\n",
      "        102       0.90      0.90      0.90        10\n",
      "        103       0.87      0.93      0.90        14\n",
      "        104       1.00      1.00      1.00         6\n",
      "        105       1.00      0.83      0.91         6\n",
      "        106       1.00      1.00      1.00         6\n",
      "        107       0.67      0.67      0.67         6\n",
      "        108       0.75      0.90      0.82        10\n",
      "        109       1.00      1.00      1.00         5\n",
      "        110       1.00      0.71      0.83         7\n",
      "        111       1.00      1.00      1.00         5\n",
      "        112       0.90      1.00      0.95         9\n",
      "        113       0.89      0.89      0.89         9\n",
      "        114       1.00      0.75      0.86         4\n",
      "        115       1.00      0.89      0.94         9\n",
      "        116       1.00      1.00      1.00        11\n",
      "        117       1.00      0.56      0.71         9\n",
      "        118       0.64      0.82      0.72        11\n",
      "        119       0.70      1.00      0.82         7\n",
      "        120       0.83      0.83      0.83         6\n",
      "        121       0.42      1.00      0.59         5\n",
      "        122       0.89      0.89      0.89         9\n",
      "        123       0.90      0.90      0.90        10\n",
      "        124       0.80      0.57      0.67         7\n",
      "        125       1.00      0.67      0.80         9\n",
      "        126       1.00      0.70      0.82        10\n",
      "        127       0.86      1.00      0.92         6\n",
      "        128       0.80      0.80      0.80         5\n",
      "        129       0.86      0.60      0.71        10\n",
      "        130       1.00      1.00      1.00        10\n",
      "        131       0.33      0.18      0.24        11\n",
      "        132       1.00      0.82      0.90        11\n",
      "        133       0.78      0.88      0.82         8\n",
      "        134       1.00      0.83      0.91         6\n",
      "        135       0.62      0.83      0.71         6\n",
      "        136       0.60      0.50      0.55         6\n",
      "        137       0.50      1.00      0.67         8\n",
      "        138       0.70      0.64      0.67        11\n",
      "        139       1.00      1.00      1.00         4\n",
      "        140       0.60      1.00      0.75         3\n",
      "        141       0.80      0.57      0.67         7\n",
      "        142       0.75      0.67      0.71         9\n",
      "        143       0.56      0.50      0.53        10\n",
      "        144       0.20      0.50      0.29         2\n",
      "        145       0.58      0.78      0.67         9\n",
      "        146       0.38      0.83      0.53         6\n",
      "        147       0.86      0.86      0.86         7\n",
      "        148       0.00      0.00      0.00         2\n",
      "        149       0.62      1.00      0.77        10\n",
      "        150       0.50      0.83      0.62         6\n",
      "        151       1.00      0.50      0.67         8\n",
      "        152       0.91      0.91      0.91        11\n",
      "        153       0.82      1.00      0.90         9\n",
      "        154       0.70      0.78      0.74         9\n",
      "        155       0.88      0.70      0.78        10\n",
      "        156       1.00      0.50      0.67        10\n",
      "        157       0.89      1.00      0.94         8\n",
      "        158       0.14      0.20      0.17         5\n",
      "        159       1.00      0.57      0.73         7\n",
      "        160       0.75      0.67      0.71         9\n",
      "        161       0.67      1.00      0.80         4\n",
      "        162       0.71      1.00      0.83         5\n",
      "        163       1.00      1.00      1.00        10\n",
      "        164       0.80      0.50      0.62         8\n",
      "        165       0.42      1.00      0.59         5\n",
      "        166       0.58      1.00      0.74         7\n",
      "        167       1.00      1.00      1.00         7\n",
      "        168       0.67      0.57      0.62        14\n",
      "        169       0.44      1.00      0.62         4\n",
      "        170       0.29      0.33      0.31         6\n",
      "        171       0.80      0.44      0.57         9\n",
      "        172       1.00      0.43      0.60         7\n",
      "        173       0.83      0.83      0.83        12\n",
      "        174       0.83      0.56      0.67         9\n",
      "        175       1.00      0.60      0.75         5\n",
      "        176       1.00      1.00      1.00         4\n",
      "        177       1.00      0.83      0.91        12\n",
      "        178       1.00      1.00      1.00        10\n",
      "        179       0.80      1.00      0.89         4\n",
      "        180       1.00      1.00      1.00        10\n",
      "        181       0.75      0.75      0.75         8\n",
      "        182       0.86      1.00      0.92         6\n",
      "        183       0.83      0.71      0.77         7\n",
      "        184       1.00      0.73      0.84        11\n",
      "        185       0.71      1.00      0.83         5\n",
      "        186       0.90      1.00      0.95         9\n",
      "        187       0.78      1.00      0.88         7\n",
      "        188       0.86      0.86      0.86         7\n",
      "        189       0.71      0.91      0.80        11\n",
      "        190       0.36      0.62      0.45         8\n",
      "        191       0.75      1.00      0.86         6\n",
      "        192       0.62      0.83      0.71         6\n",
      "        193       1.00      0.23      0.38        13\n",
      "        194       0.36      0.90      0.51        10\n",
      "        195       0.56      0.75      0.64        12\n",
      "        196       0.70      0.58      0.64        12\n",
      "        197       0.82      1.00      0.90         9\n",
      "        198       1.00      0.90      0.95        10\n",
      "        199       0.38      0.43      0.40         7\n",
      "        200       1.00      0.91      0.95        11\n",
      "        201       0.29      0.29      0.29         7\n",
      "        202       1.00      1.00      1.00        12\n",
      "        203       1.00      0.75      0.86         8\n",
      "        204       0.33      0.60      0.43         5\n",
      "        205       0.56      1.00      0.71         5\n",
      "        206       0.86      0.67      0.75         9\n",
      "        207       0.58      0.64      0.61        11\n",
      "        208       0.75      1.00      0.86         6\n",
      "        209       0.60      1.00      0.75         6\n",
      "        210       1.00      0.25      0.40         4\n",
      "        211       1.00      0.78      0.88         9\n",
      "        212       0.50      0.43      0.46         7\n",
      "        213       0.25      0.40      0.31         5\n",
      "        214       0.64      1.00      0.78         7\n",
      "        215       0.89      0.89      0.89         9\n",
      "        216       0.80      1.00      0.89         4\n",
      "        217       0.60      0.43      0.50         7\n",
      "        218       0.85      0.65      0.73        17\n",
      "        219       0.75      0.67      0.71         9\n",
      "        220       1.00      0.89      0.94         9\n",
      "        221       1.00      0.46      0.63        13\n",
      "        222       0.82      0.90      0.86        10\n",
      "        223       0.80      0.89      0.84         9\n",
      "        224       1.00      0.33      0.50         3\n",
      "        225       0.46      0.60      0.52        10\n",
      "        226       0.50      0.60      0.55         5\n",
      "        227       1.00      0.56      0.71         9\n",
      "        228       1.00      0.67      0.80         6\n",
      "        229       1.00      0.75      0.86         4\n",
      "        230       1.00      0.40      0.57         5\n",
      "        231       1.00      0.88      0.93         8\n",
      "        232       1.00      0.58      0.74        12\n",
      "        233       1.00      0.75      0.86         8\n",
      "        234       0.90      0.90      0.90        10\n",
      "        235       0.82      1.00      0.90         9\n",
      "        236       1.00      1.00      1.00         7\n",
      "        237       0.47      1.00      0.64         8\n",
      "        238       1.00      0.83      0.91         6\n",
      "        239       0.91      0.83      0.87        12\n",
      "        240       0.42      0.83      0.56         6\n",
      "        241       0.80      0.73      0.76        11\n",
      "        242       0.83      0.71      0.77         7\n",
      "        243       0.57      0.67      0.62        12\n",
      "        244       1.00      1.00      1.00         6\n",
      "        245       0.90      1.00      0.95         9\n",
      "        246       0.67      0.67      0.67         9\n",
      "        247       0.91      0.77      0.83        13\n",
      "        248       0.67      0.75      0.71         8\n",
      "        249       1.00      0.92      0.96        12\n",
      "        250       1.00      1.00      1.00        10\n",
      "        251       0.80      0.40      0.53        10\n",
      "        252       1.00      0.71      0.83         7\n",
      "        253       1.00      1.00      1.00        13\n",
      "        254       1.00      0.67      0.80         9\n",
      "        255       0.62      0.62      0.62         8\n",
      "        256       0.90      0.90      0.90        10\n",
      "        257       0.86      0.86      0.86         7\n",
      "        258       0.89      0.89      0.89         9\n",
      "        259       0.56      0.83      0.67         6\n",
      "        260       1.00      0.86      0.92         7\n",
      "        261       0.86      0.86      0.86         7\n",
      "        262       1.00      0.20      0.33         5\n",
      "        263       1.00      0.75      0.86         8\n",
      "        264       0.67      0.89      0.76         9\n",
      "        265       1.00      0.33      0.50         6\n",
      "        266       1.00      0.88      0.93         8\n",
      "        267       0.88      1.00      0.93         7\n",
      "        268       0.85      1.00      0.92        11\n",
      "        269       0.89      0.80      0.84        10\n",
      "        270       1.00      0.88      0.93         8\n",
      "        271       0.91      0.91      0.91        11\n",
      "        272       0.79      1.00      0.88        11\n",
      "        273       1.00      0.71      0.83         7\n",
      "        274       1.00      0.40      0.57         5\n",
      "        275       0.80      0.80      0.80         5\n",
      "        276       0.00      0.00      0.00         5\n",
      "        277       0.83      0.56      0.67         9\n",
      "        278       0.71      0.86      0.77        14\n",
      "        279       0.75      0.50      0.60         6\n",
      "        280       0.78      1.00      0.88         7\n",
      "        281       0.78      1.00      0.88         7\n",
      "        282       0.80      0.67      0.73        12\n",
      "        283       1.00      0.80      0.89        10\n",
      "        284       1.00      0.83      0.91        12\n",
      "        285       0.71      0.71      0.71         7\n",
      "        286       1.00      0.78      0.88         9\n",
      "        287       0.67      0.75      0.71         8\n",
      "        288       0.50      0.80      0.62         5\n",
      "        289       0.80      0.89      0.84         9\n",
      "        290       1.00      0.89      0.94         9\n",
      "        291       0.71      0.56      0.63         9\n",
      "        292       0.75      1.00      0.86         6\n",
      "        293       1.00      1.00      1.00         9\n",
      "        294       0.75      0.60      0.67         5\n",
      "        295       0.29      0.22      0.25         9\n",
      "        296       0.89      0.89      0.89         9\n",
      "        297       0.75      0.38      0.50         8\n",
      "        298       0.25      0.09      0.13        11\n",
      "        299       0.73      0.79      0.76        14\n",
      "        300       0.60      0.60      0.60         5\n",
      "        301       1.00      0.57      0.73         7\n",
      "        302       0.78      0.58      0.67        12\n",
      "        303       0.78      0.78      0.78         9\n",
      "        304       0.63      0.86      0.73        14\n",
      "        305       0.83      0.71      0.77         7\n",
      "        306       1.00      0.83      0.91         6\n",
      "        307       0.67      0.86      0.75         7\n",
      "        308       1.00      0.33      0.50         6\n",
      "        309       0.60      0.90      0.72        10\n",
      "        310       0.85      0.92      0.88        12\n",
      "        311       0.38      0.33      0.35         9\n",
      "        312       0.67      0.29      0.40         7\n",
      "        313       0.53      1.00      0.69        10\n",
      "        314       1.00      0.90      0.95        10\n",
      "        315       0.91      0.83      0.87        12\n",
      "        316       1.00      0.88      0.93         8\n",
      "        317       0.89      0.89      0.89         9\n",
      "        318       0.71      0.91      0.80        11\n",
      "        319       0.78      1.00      0.88         7\n",
      "        320       0.86      0.86      0.86         7\n",
      "        321       1.00      0.75      0.86         4\n",
      "        322       1.00      0.83      0.91         6\n",
      "        323       0.57      0.67      0.62         6\n",
      "        324       0.83      0.71      0.77         7\n",
      "        325       0.64      0.88      0.74         8\n",
      "        326       1.00      1.00      1.00         7\n",
      "        327       0.88      0.88      0.88         8\n",
      "        328       0.57      0.57      0.57         7\n",
      "        329       0.89      0.67      0.76        12\n",
      "        330       0.64      0.82      0.72        11\n",
      "        331       1.00      0.60      0.75         5\n",
      "        332       1.00      0.50      0.67         6\n",
      "        333       1.00      0.92      0.96        13\n",
      "        334       0.88      0.58      0.70        12\n",
      "        335       0.90      1.00      0.95         9\n",
      "        336       0.88      0.88      0.88         8\n",
      "        337       1.00      0.90      0.95        10\n",
      "        338       0.56      0.90      0.69        10\n",
      "        339       1.00      0.80      0.89        10\n",
      "        340       0.83      1.00      0.91         5\n",
      "        341       0.60      1.00      0.75         3\n",
      "        342       0.92      1.00      0.96        11\n",
      "        343       0.71      0.83      0.77         6\n",
      "        344       1.00      0.94      0.97        17\n",
      "        345       0.88      0.88      0.88         8\n",
      "        346       0.78      0.78      0.78         9\n",
      "        347       1.00      0.90      0.95        10\n",
      "        348       1.00      0.88      0.93         8\n",
      "        349       0.62      1.00      0.77         5\n",
      "        350       0.50      0.89      0.64         9\n",
      "        351       0.83      0.83      0.83         6\n",
      "        352       0.70      0.88      0.78         8\n",
      "        353       1.00      0.83      0.91         6\n",
      "        354       0.57      1.00      0.73         4\n",
      "        355       1.00      0.67      0.80         9\n",
      "        356       0.60      0.27      0.37        11\n",
      "        357       0.62      1.00      0.77         5\n",
      "        358       1.00      0.64      0.78        11\n",
      "        359       1.00      0.83      0.91         6\n",
      "        360       1.00      0.78      0.88         9\n",
      "        361       1.00      1.00      1.00         7\n",
      "        362       0.67      0.44      0.53         9\n",
      "        363       1.00      0.56      0.71         9\n",
      "        364       0.57      0.80      0.67         5\n",
      "        365       0.82      0.90      0.86        10\n",
      "        366       0.90      0.90      0.90        10\n",
      "        367       1.00      0.80      0.89        10\n",
      "        368       1.00      0.91      0.95        11\n",
      "        369       0.80      0.57      0.67         7\n",
      "        370       1.00      1.00      1.00        10\n",
      "        371       0.78      0.78      0.78         9\n",
      "        372       0.82      0.75      0.78        12\n",
      "        373       1.00      0.40      0.57         5\n",
      "        374       0.77      0.91      0.83        11\n",
      "        375       0.67      1.00      0.80         8\n",
      "        376       0.86      0.60      0.71        10\n",
      "        377       1.00      0.57      0.73         7\n",
      "        378       1.00      0.58      0.74        12\n",
      "        379       0.91      1.00      0.95        10\n",
      "        380       1.00      0.75      0.86         4\n",
      "        381       0.33      0.50      0.40         4\n",
      "        382       0.75      0.90      0.82        10\n",
      "        383       0.55      1.00      0.71         6\n",
      "        384       0.44      0.80      0.57         5\n",
      "        385       0.33      0.33      0.33         3\n",
      "        386       0.75      0.75      0.75         8\n",
      "        387       1.00      0.80      0.89        10\n",
      "        388       1.00      0.73      0.84        11\n",
      "        389       0.82      0.75      0.78        12\n",
      "        390       0.40      0.25      0.31         8\n",
      "        391       1.00      0.88      0.93         8\n",
      "        392       0.80      1.00      0.89         8\n",
      "        393       0.50      0.50      0.50         2\n",
      "        394       0.43      0.67      0.52         9\n",
      "        395       0.80      0.57      0.67         7\n",
      "        396       0.82      1.00      0.90         9\n",
      "        397       0.91      0.91      0.91        11\n",
      "        398       0.60      0.60      0.60         5\n",
      "        399       1.00      1.00      1.00         3\n",
      "        400       0.86      0.75      0.80         8\n",
      "        401       0.75      0.67      0.71         9\n",
      "        402       1.00      0.57      0.73         7\n",
      "        403       1.00      1.00      1.00         9\n",
      "        404       0.64      1.00      0.78         7\n",
      "        405       0.86      1.00      0.92         6\n",
      "        406       0.75      0.60      0.67         5\n",
      "        407       0.86      0.86      0.86         7\n",
      "        408       1.00      0.88      0.93         8\n",
      "        409       0.67      0.67      0.67         3\n",
      "        410       0.67      1.00      0.80         6\n",
      "        411       1.00      0.56      0.71         9\n",
      "        412       0.00      0.00      0.00         7\n",
      "        413       0.75      0.38      0.50         8\n",
      "        414       0.80      1.00      0.89         4\n",
      "        415       0.50      0.57      0.53         7\n",
      "        416       0.88      1.00      0.93         7\n",
      "        417       1.00      1.00      1.00         6\n",
      "        418       0.60      0.43      0.50         7\n",
      "        419       0.86      0.86      0.86         7\n",
      "        420       1.00      0.75      0.86         4\n",
      "        421       0.89      0.80      0.84        10\n",
      "        422       0.80      1.00      0.89         4\n",
      "        423       0.60      0.38      0.46         8\n",
      "        424       0.17      0.50      0.25         2\n",
      "        425       1.00      0.40      0.57        10\n",
      "        426       1.00      1.00      1.00         7\n",
      "        427       1.00      0.43      0.60         7\n",
      "        428       0.91      1.00      0.95        10\n",
      "        429       0.67      0.29      0.40         7\n",
      "        430       1.00      1.00      1.00        10\n",
      "        431       1.00      0.80      0.89         5\n",
      "        432       0.94      1.00      0.97        15\n",
      "        433       0.73      0.89      0.80         9\n",
      "        434       1.00      0.88      0.93         8\n",
      "        435       0.71      1.00      0.83         5\n",
      "        436       1.00      1.00      1.00         3\n",
      "        437       1.00      0.88      0.93         8\n",
      "        438       1.00      0.91      0.95        11\n",
      "        439       1.00      1.00      1.00         4\n",
      "        440       0.56      0.82      0.67        11\n",
      "        441       0.91      1.00      0.95        10\n",
      "        442       1.00      0.88      0.93         8\n",
      "        443       1.00      0.89      0.94         9\n",
      "        444       1.00      0.80      0.89         5\n",
      "        445       1.00      1.00      1.00        12\n",
      "        446       1.00      0.83      0.91         6\n",
      "        447       1.00      0.82      0.90        11\n",
      "        448       0.50      0.43      0.46         7\n",
      "        449       0.80      1.00      0.89         8\n",
      "        450       1.00      0.86      0.92         7\n",
      "        451       1.00      0.77      0.87        13\n",
      "        452       0.83      0.83      0.83         6\n",
      "        453       1.00      1.00      1.00        10\n",
      "        454       0.62      1.00      0.77         5\n",
      "        455       0.88      0.88      0.88         8\n",
      "        456       0.71      0.62      0.67         8\n",
      "        457       0.71      0.83      0.77         6\n",
      "        458       0.67      0.57      0.62         7\n",
      "        459       0.50      0.11      0.18         9\n",
      "        460       1.00      0.80      0.89         5\n",
      "        461       0.83      0.83      0.83         6\n",
      "        462       1.00      1.00      1.00        12\n",
      "        463       1.00      1.00      1.00        12\n",
      "        464       1.00      0.42      0.59        12\n",
      "        465       1.00      0.89      0.94         9\n",
      "        466       0.80      1.00      0.89         8\n",
      "        467       0.83      0.83      0.83         6\n",
      "        468       0.38      0.75      0.50         4\n",
      "        469       1.00      1.00      1.00         8\n",
      "        470       0.92      1.00      0.96        12\n",
      "        471       1.00      1.00      1.00         3\n",
      "        472       0.83      0.71      0.77         7\n",
      "        473       1.00      0.80      0.89        10\n",
      "        474       0.50      1.00      0.67         6\n",
      "        475       1.00      1.00      1.00         5\n",
      "        476       1.00      1.00      1.00         4\n",
      "        477       1.00      1.00      1.00         8\n",
      "        478       1.00      1.00      1.00        10\n",
      "        479       1.00      0.86      0.92         7\n",
      "        480       1.00      0.83      0.91         6\n",
      "        481       0.88      1.00      0.93         7\n",
      "        482       0.62      0.83      0.71         6\n",
      "        483       0.75      0.27      0.40        11\n",
      "        484       0.62      0.71      0.67         7\n",
      "        485       0.78      0.88      0.82         8\n",
      "        486       1.00      1.00      1.00         7\n",
      "        487       1.00      0.78      0.88         9\n",
      "        488       0.62      0.83      0.71         6\n",
      "        489       1.00      0.86      0.92         7\n",
      "        490       1.00      0.90      0.95        10\n",
      "        491       1.00      0.90      0.95        10\n",
      "        492       1.00      1.00      1.00         2\n",
      "        493       0.57      0.80      0.67         5\n",
      "        494       0.59      0.83      0.69        12\n",
      "        495       1.00      1.00      1.00         8\n",
      "        496       0.93      1.00      0.96        13\n",
      "        497       1.00      1.00      1.00         9\n",
      "        498       0.88      0.88      0.88         8\n",
      "        499       0.64      1.00      0.78         7\n",
      "        500       1.00      1.00      1.00         9\n",
      "        501       1.00      1.00      1.00         8\n",
      "        502       0.67      0.60      0.63        10\n",
      "        503       0.89      0.80      0.84        10\n",
      "        504       1.00      1.00      1.00         7\n",
      "        505       0.53      0.82      0.64        11\n",
      "        506       1.00      1.00      1.00         3\n",
      "        507       0.77      1.00      0.87        10\n",
      "        508       1.00      0.88      0.93         8\n",
      "        509       0.70      1.00      0.82         7\n",
      "        510       0.67      0.80      0.73        10\n",
      "        511       0.00      0.00      0.00        10\n",
      "        512       0.43      0.60      0.50         5\n",
      "        513       1.00      1.00      1.00         8\n",
      "        514       0.88      0.88      0.88         8\n",
      "        515       1.00      0.83      0.91         6\n",
      "        516       0.86      0.75      0.80         8\n",
      "        517       1.00      1.00      1.00         3\n",
      "        518       1.00      0.78      0.88         9\n",
      "        519       0.71      1.00      0.83        10\n",
      "        520       0.50      1.00      0.67         3\n",
      "        521       1.00      1.00      1.00         6\n",
      "        522       1.00      0.71      0.83         7\n",
      "        523       1.00      0.60      0.75         5\n",
      "        524       1.00      1.00      1.00         6\n",
      "        525       1.00      1.00      1.00        12\n",
      "        526       0.71      0.56      0.63         9\n",
      "        527       1.00      1.00      1.00         9\n",
      "        528       0.91      0.91      0.91        11\n",
      "        529       0.90      1.00      0.95         9\n",
      "        530       0.75      0.86      0.80         7\n",
      "        531       1.00      0.62      0.77         8\n",
      "        532       0.60      0.50      0.55         6\n",
      "        533       0.50      0.71      0.59         7\n",
      "        534       0.75      0.90      0.82        10\n",
      "        535       0.67      1.00      0.80         6\n",
      "        536       0.80      1.00      0.89         4\n",
      "\n",
      "avg / total       0.82      0.79      0.79      4296\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['label'], test_df['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 0 0 ... 0 0 0]\n",
      " [0 8 1 ... 0 0 0]\n",
      " [0 0 4 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 9 0 0]\n",
      " [0 0 0 ... 0 6 0]\n",
      " [0 0 0 ... 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_df['label'], test_df['predict']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/his/train.tsv\", sep='\\t')\n",
    "dev_df = pd.read_csv(\"../data/his/dev.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(\"../data/his/test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libmecab-dev is already the newest version (0.996-1.2ubuntu1).\n",
      "mecab is already the newest version (0.996-1.2ubuntu1).\n",
      "mecab-ipadic is already the newest version (2.7.0-20070801+main-1).\n",
      "mecab-ipadic-utf8 is already the newest version (2.7.0-20070801+main-1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libnvidia-container-tools libnvidia-container1 nvidia-container-runtime\n",
      "  nvidia-container-runtime-hook\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -q -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3==0.7 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (0.7)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3==0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MeCab.Tagger(\"-Owakati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_df = pd.concat([train_df, dev_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_xs = train_dev_df['text'].apply(lambda x: m.parse(x))\n",
    "train_dev_ys = train_dev_df['label']\n",
    "\n",
    "test_xs = test_df['text'].apply(lambda x: m.parse(x))\n",
    "test_ys = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=750)\n",
    "train_dev_xs_ = vectorizer.fit_transform(train_dev_xs)\n",
    "test_xs_ = vectorizer.transform(test_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set up is not exactly identical to that of BERT because inside Classifier it uses `train_test_split` with shuffle.  \n",
    "In addition, parameters are not well tuned, however, we think it's enough to check the power of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 248 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# model = GradientBoostingClassifier(n_estimators=200,\n",
    "#                                    validation_fraction=len(train_df)/len(dev_df),\n",
    "#                                    n_iter_no_change=5,\n",
    "#                                    tol=0.01,\n",
    "#                                    random_state=23)\n",
    "\n",
    "### 1/5 of full training data.\n",
    "# model = GradientBoostingClassifier(n_estimators=200,\n",
    "#                                    validation_fraction=len(dev_df)/len(train_df),\n",
    "#                                    n_iter_no_change=5,\n",
    "#                                    tol=0.01,\n",
    "#                                    random_state=23)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "model.fit(train_dev_xs_, train_dev_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88         8\n",
      "          1       0.89      0.89      0.89         9\n",
      "          2       0.75      0.60      0.67         5\n",
      "          3       0.17      0.50      0.25         2\n",
      "          4       0.88      0.64      0.74        11\n",
      "          5       1.00      1.00      1.00         5\n",
      "          6       0.86      0.75      0.80         8\n",
      "          7       1.00      0.86      0.92         7\n",
      "          8       0.88      0.70      0.78        10\n",
      "          9       0.75      1.00      0.86         6\n",
      "         10       0.89      0.73      0.80        11\n",
      "         11       0.62      0.83      0.71         6\n",
      "         12       0.89      0.73      0.80        11\n",
      "         13       0.86      0.55      0.67        11\n",
      "         14       0.88      0.88      0.88         8\n",
      "         15       0.80      0.80      0.80         5\n",
      "         16       0.83      0.62      0.71         8\n",
      "         17       0.50      0.57      0.53         7\n",
      "         18       1.00      0.86      0.92         7\n",
      "         19       0.62      0.71      0.67         7\n",
      "         20       0.67      0.80      0.73        10\n",
      "         21       0.67      0.29      0.40         7\n",
      "         22       0.67      0.67      0.67         6\n",
      "         23       0.75      0.55      0.63        11\n",
      "         24       0.67      1.00      0.80         4\n",
      "         25       0.91      1.00      0.95        10\n",
      "         26       1.00      1.00      1.00         6\n",
      "         27       0.67      0.67      0.67         9\n",
      "         28       0.67      0.73      0.70        11\n",
      "         29       1.00      0.89      0.94         9\n",
      "         30       0.73      0.89      0.80         9\n",
      "         31       1.00      0.75      0.86         8\n",
      "         32       0.67      0.57      0.62         7\n",
      "         33       0.25      0.14      0.18         7\n",
      "         34       0.67      0.40      0.50        10\n",
      "         35       0.77      0.83      0.80        12\n",
      "         36       0.62      0.89      0.73         9\n",
      "         37       0.50      0.25      0.33         4\n",
      "         38       0.86      0.67      0.75         9\n",
      "         39       1.00      0.83      0.91         6\n",
      "         40       0.88      0.88      0.88         8\n",
      "         41       0.46      0.75      0.57         8\n",
      "         42       0.50      0.33      0.40         6\n",
      "         43       1.00      1.00      1.00         7\n",
      "         44       0.88      0.78      0.82         9\n",
      "         45       0.86      1.00      0.92         6\n",
      "         46       0.92      0.92      0.92        12\n",
      "         47       1.00      0.88      0.93         8\n",
      "         48       0.64      0.78      0.70         9\n",
      "         49       0.88      1.00      0.93         7\n",
      "         50       1.00      0.29      0.44         7\n",
      "         51       0.50      0.78      0.61         9\n",
      "         52       0.80      0.80      0.80         5\n",
      "         53       0.86      1.00      0.92         6\n",
      "         54       0.33      0.50      0.40         6\n",
      "         55       0.75      0.75      0.75        12\n",
      "         56       0.50      0.25      0.33         8\n",
      "         57       0.40      0.57      0.47         7\n",
      "         58       0.89      0.89      0.89         9\n",
      "         59       0.33      1.00      0.50         3\n",
      "         60       0.85      1.00      0.92        11\n",
      "         61       0.75      0.75      0.75         4\n",
      "         62       1.00      0.86      0.92        14\n",
      "         63       0.80      0.67      0.73         6\n",
      "         64       1.00      0.71      0.83         7\n",
      "         65       1.00      0.56      0.71         9\n",
      "         66       0.75      1.00      0.86         6\n",
      "         67       0.58      0.58      0.58        12\n",
      "         68       0.50      0.75      0.60         4\n",
      "         69       0.86      1.00      0.92         6\n",
      "         70       0.27      0.50      0.35         6\n",
      "         71       0.67      0.40      0.50        10\n",
      "         72       0.71      0.50      0.59        10\n",
      "         73       0.24      0.56      0.33         9\n",
      "         74       1.00      0.75      0.86         8\n",
      "         75       1.00      0.69      0.82        13\n",
      "         76       0.86      0.75      0.80         8\n",
      "         77       0.57      0.40      0.47        10\n",
      "         78       0.75      1.00      0.86         6\n",
      "         79       0.83      1.00      0.91         5\n",
      "         80       0.50      0.50      0.50         8\n",
      "         81       1.00      0.88      0.93         8\n",
      "         82       0.50      0.33      0.40         3\n",
      "         83       0.91      0.91      0.91        11\n",
      "         84       0.67      1.00      0.80         4\n",
      "         85       0.80      0.89      0.84         9\n",
      "         86       0.83      0.91      0.87        11\n",
      "         87       1.00      0.90      0.95        10\n",
      "         88       0.78      1.00      0.88         7\n",
      "         89       0.80      0.80      0.80        10\n",
      "         90       0.50      0.86      0.63         7\n",
      "         91       0.67      0.86      0.75         7\n",
      "         92       1.00      0.80      0.89         5\n",
      "         93       0.67      0.73      0.70        11\n",
      "         94       0.33      0.25      0.29         8\n",
      "         95       0.89      0.89      0.89         9\n",
      "         96       0.80      0.62      0.70        13\n",
      "         97       0.78      0.78      0.78         9\n",
      "         98       0.83      1.00      0.91        10\n",
      "         99       1.00      0.80      0.89         5\n",
      "        100       0.60      0.60      0.60         5\n",
      "        101       1.00      1.00      1.00        10\n",
      "        102       0.82      0.90      0.86        10\n",
      "        103       0.91      0.71      0.80        14\n",
      "        104       0.86      1.00      0.92         6\n",
      "        105       0.67      0.67      0.67         6\n",
      "        106       1.00      0.83      0.91         6\n",
      "        107       0.40      0.67      0.50         6\n",
      "        108       0.73      0.80      0.76        10\n",
      "        109       0.80      0.80      0.80         5\n",
      "        110       1.00      0.57      0.73         7\n",
      "        111       0.50      0.60      0.55         5\n",
      "        112       0.89      0.89      0.89         9\n",
      "        113       0.89      0.89      0.89         9\n",
      "        114       0.50      0.50      0.50         4\n",
      "        115       0.80      0.89      0.84         9\n",
      "        116       1.00      1.00      1.00        11\n",
      "        117       1.00      1.00      1.00         9\n",
      "        118       0.78      0.64      0.70        11\n",
      "        119       0.88      1.00      0.93         7\n",
      "        120       0.14      0.17      0.15         6\n",
      "        121       0.44      0.80      0.57         5\n",
      "        122       0.80      0.89      0.84         9\n",
      "        123       0.75      0.60      0.67        10\n",
      "        124       0.60      0.86      0.71         7\n",
      "        125       0.80      0.89      0.84         9\n",
      "        126       0.78      0.70      0.74        10\n",
      "        127       0.75      1.00      0.86         6\n",
      "        128       0.50      0.80      0.62         5\n",
      "        129       0.80      0.80      0.80        10\n",
      "        130       0.82      0.90      0.86        10\n",
      "        131       0.50      0.09      0.15        11\n",
      "        132       1.00      0.73      0.84        11\n",
      "        133       0.67      1.00      0.80         8\n",
      "        134       0.40      0.67      0.50         6\n",
      "        135       0.38      0.50      0.43         6\n",
      "        136       0.50      0.50      0.50         6\n",
      "        137       0.38      0.38      0.38         8\n",
      "        138       0.56      0.45      0.50        11\n",
      "        139       0.33      0.50      0.40         4\n",
      "        140       0.25      0.33      0.29         3\n",
      "        141       0.67      0.57      0.62         7\n",
      "        142       0.64      0.78      0.70         9\n",
      "        143       0.73      0.80      0.76        10\n",
      "        144       0.33      0.50      0.40         2\n",
      "        145       0.75      0.67      0.71         9\n",
      "        146       0.17      0.17      0.17         6\n",
      "        147       0.86      0.86      0.86         7\n",
      "        148       0.00      0.00      0.00         2\n",
      "        149       0.88      0.70      0.78        10\n",
      "        150       0.62      0.83      0.71         6\n",
      "        151       0.83      0.62      0.71         8\n",
      "        152       0.57      0.36      0.44        11\n",
      "        153       0.82      1.00      0.90         9\n",
      "        154       0.89      0.89      0.89         9\n",
      "        155       0.70      0.70      0.70        10\n",
      "        156       0.83      0.50      0.62        10\n",
      "        157       0.40      0.25      0.31         8\n",
      "        158       0.17      0.20      0.18         5\n",
      "        159       1.00      0.86      0.92         7\n",
      "        160       0.80      0.89      0.84         9\n",
      "        161       0.43      0.75      0.55         4\n",
      "        162       0.56      1.00      0.71         5\n",
      "        163       0.73      0.80      0.76        10\n",
      "        164       0.86      0.75      0.80         8\n",
      "        165       0.67      0.80      0.73         5\n",
      "        166       0.78      1.00      0.88         7\n",
      "        167       0.88      1.00      0.93         7\n",
      "        168       0.78      0.50      0.61        14\n",
      "        169       0.33      0.75      0.46         4\n",
      "        170       0.50      0.50      0.50         6\n",
      "        171       0.86      0.67      0.75         9\n",
      "        172       0.50      0.57      0.53         7\n",
      "        173       0.83      0.83      0.83        12\n",
      "        174       0.71      0.56      0.63         9\n",
      "        175       0.83      1.00      0.91         5\n",
      "        176       1.00      1.00      1.00         4\n",
      "        177       0.79      0.92      0.85        12\n",
      "        178       1.00      0.90      0.95        10\n",
      "        179       0.80      1.00      0.89         4\n",
      "        180       1.00      1.00      1.00        10\n",
      "        181       1.00      0.75      0.86         8\n",
      "        182       0.86      1.00      0.92         6\n",
      "        183       0.83      0.71      0.77         7\n",
      "        184       1.00      0.64      0.78        11\n",
      "        185       0.56      1.00      0.71         5\n",
      "        186       0.73      0.89      0.80         9\n",
      "        187       0.75      0.43      0.55         7\n",
      "        188       0.71      0.71      0.71         7\n",
      "        189       0.82      0.82      0.82        11\n",
      "        190       1.00      0.62      0.77         8\n",
      "        191       0.33      0.50      0.40         6\n",
      "        192       0.71      0.83      0.77         6\n",
      "        193       0.64      0.54      0.58        13\n",
      "        194       0.60      0.90      0.72        10\n",
      "        195       0.80      0.67      0.73        12\n",
      "        196       0.67      0.67      0.67        12\n",
      "        197       0.75      1.00      0.86         9\n",
      "        198       1.00      0.90      0.95        10\n",
      "        199       0.33      0.14      0.20         7\n",
      "        200       1.00      0.73      0.84        11\n",
      "        201       0.43      0.43      0.43         7\n",
      "        202       1.00      1.00      1.00        12\n",
      "        203       0.88      0.88      0.88         8\n",
      "        204       0.43      0.60      0.50         5\n",
      "        205       1.00      0.80      0.89         5\n",
      "        206       0.86      0.67      0.75         9\n",
      "        207       0.73      0.73      0.73        11\n",
      "        208       0.62      0.83      0.71         6\n",
      "        209       0.56      0.83      0.67         6\n",
      "        210       1.00      1.00      1.00         4\n",
      "        211       0.75      0.33      0.46         9\n",
      "        212       1.00      0.43      0.60         7\n",
      "        213       0.33      0.60      0.43         5\n",
      "        214       0.67      0.86      0.75         7\n",
      "        215       0.89      0.89      0.89         9\n",
      "        216       1.00      1.00      1.00         4\n",
      "        217       0.67      0.57      0.62         7\n",
      "        218       1.00      0.71      0.83        17\n",
      "        219       0.90      1.00      0.95         9\n",
      "        220       0.80      0.89      0.84         9\n",
      "        221       0.90      0.69      0.78        13\n",
      "        222       0.70      0.70      0.70        10\n",
      "        223       0.80      0.89      0.84         9\n",
      "        224       0.75      1.00      0.86         3\n",
      "        225       0.60      0.30      0.40        10\n",
      "        226       0.60      0.60      0.60         5\n",
      "        227       0.50      0.67      0.57         9\n",
      "        228       0.80      0.67      0.73         6\n",
      "        229       1.00      0.75      0.86         4\n",
      "        230       1.00      0.60      0.75         5\n",
      "        231       0.83      0.62      0.71         8\n",
      "        232       0.92      0.92      0.92        12\n",
      "        233       1.00      0.75      0.86         8\n",
      "        234       0.90      0.90      0.90        10\n",
      "        235       0.88      0.78      0.82         9\n",
      "        236       1.00      1.00      1.00         7\n",
      "        237       0.70      0.88      0.78         8\n",
      "        238       1.00      0.83      0.91         6\n",
      "        239       0.69      0.75      0.72        12\n",
      "        240       0.56      0.83      0.67         6\n",
      "        241       0.88      0.64      0.74        11\n",
      "        242       0.67      0.86      0.75         7\n",
      "        243       0.58      0.58      0.58        12\n",
      "        244       0.83      0.83      0.83         6\n",
      "        245       1.00      1.00      1.00         9\n",
      "        246       0.75      0.67      0.71         9\n",
      "        247       0.69      0.85      0.76        13\n",
      "        248       0.86      0.75      0.80         8\n",
      "        249       1.00      0.92      0.96        12\n",
      "        250       0.80      0.80      0.80        10\n",
      "        251       0.83      0.50      0.62        10\n",
      "        252       1.00      0.71      0.83         7\n",
      "        253       0.80      0.92      0.86        13\n",
      "        254       0.88      0.78      0.82         9\n",
      "        255       0.57      0.50      0.53         8\n",
      "        256       1.00      0.80      0.89        10\n",
      "        257       0.83      0.71      0.77         7\n",
      "        258       1.00      1.00      1.00         9\n",
      "        259       0.62      0.83      0.71         6\n",
      "        260       1.00      0.86      0.92         7\n",
      "        261       0.71      0.71      0.71         7\n",
      "        262       0.38      0.60      0.46         5\n",
      "        263       1.00      0.62      0.77         8\n",
      "        264       0.67      0.44      0.53         9\n",
      "        265       0.67      0.67      0.67         6\n",
      "        266       1.00      0.88      0.93         8\n",
      "        267       0.88      1.00      0.93         7\n",
      "        268       1.00      0.82      0.90        11\n",
      "        269       1.00      0.80      0.89        10\n",
      "        270       0.88      0.88      0.88         8\n",
      "        271       0.75      0.82      0.78        11\n",
      "        272       0.75      0.82      0.78        11\n",
      "        273       0.50      0.43      0.46         7\n",
      "        274       0.42      1.00      0.59         5\n",
      "        275       0.80      0.80      0.80         5\n",
      "        276       0.62      1.00      0.77         5\n",
      "        277       0.75      0.67      0.71         9\n",
      "        278       1.00      0.57      0.73        14\n",
      "        279       0.62      0.83      0.71         6\n",
      "        280       0.50      0.57      0.53         7\n",
      "        281       0.78      1.00      0.88         7\n",
      "        282       0.80      0.67      0.73        12\n",
      "        283       0.71      0.50      0.59        10\n",
      "        284       0.79      0.92      0.85        12\n",
      "        285       0.58      1.00      0.74         7\n",
      "        286       1.00      0.89      0.94         9\n",
      "        287       0.50      0.62      0.56         8\n",
      "        288       0.50      0.40      0.44         5\n",
      "        289       0.83      0.56      0.67         9\n",
      "        290       0.90      1.00      0.95         9\n",
      "        291       0.70      0.78      0.74         9\n",
      "        292       0.86      1.00      0.92         6\n",
      "        293       0.73      0.89      0.80         9\n",
      "        294       0.50      0.80      0.62         5\n",
      "        295       0.45      0.56      0.50         9\n",
      "        296       1.00      0.78      0.88         9\n",
      "        297       0.33      0.12      0.18         8\n",
      "        298       0.78      0.64      0.70        11\n",
      "        299       1.00      0.86      0.92        14\n",
      "        300       0.67      0.40      0.50         5\n",
      "        301       0.75      0.86      0.80         7\n",
      "        302       0.86      0.50      0.63        12\n",
      "        303       1.00      0.56      0.71         9\n",
      "        304       0.80      0.86      0.83        14\n",
      "        305       0.71      0.71      0.71         7\n",
      "        306       0.71      0.83      0.77         6\n",
      "        307       0.75      0.86      0.80         7\n",
      "        308       0.75      1.00      0.86         6\n",
      "        309       0.90      0.90      0.90        10\n",
      "        310       0.92      1.00      0.96        12\n",
      "        311       0.08      0.11      0.10         9\n",
      "        312       0.62      0.71      0.67         7\n",
      "        313       1.00      0.80      0.89        10\n",
      "        314       0.90      0.90      0.90        10\n",
      "        315       1.00      0.75      0.86        12\n",
      "        316       1.00      0.75      0.86         8\n",
      "        317       0.70      0.78      0.74         9\n",
      "        318       0.77      0.91      0.83        11\n",
      "        319       0.75      0.86      0.80         7\n",
      "        320       0.75      0.43      0.55         7\n",
      "        321       0.67      1.00      0.80         4\n",
      "        322       0.75      1.00      0.86         6\n",
      "        323       0.83      0.83      0.83         6\n",
      "        324       0.86      0.86      0.86         7\n",
      "        325       0.55      0.75      0.63         8\n",
      "        326       1.00      0.86      0.92         7\n",
      "        327       0.64      0.88      0.74         8\n",
      "        328       0.62      0.71      0.67         7\n",
      "        329       0.71      0.83      0.77        12\n",
      "        330       0.67      0.55      0.60        11\n",
      "        331       0.80      0.80      0.80         5\n",
      "        332       0.83      0.83      0.83         6\n",
      "        333       0.92      0.92      0.92        13\n",
      "        334       0.88      0.58      0.70        12\n",
      "        335       0.67      0.89      0.76         9\n",
      "        336       1.00      0.62      0.77         8\n",
      "        337       0.80      0.80      0.80        10\n",
      "        338       0.75      0.60      0.67        10\n",
      "        339       0.57      0.40      0.47        10\n",
      "        340       0.50      0.40      0.44         5\n",
      "        341       0.22      0.67      0.33         3\n",
      "        342       0.92      1.00      0.96        11\n",
      "        343       1.00      0.67      0.80         6\n",
      "        344       1.00      0.94      0.97        17\n",
      "        345       1.00      0.88      0.93         8\n",
      "        346       0.55      0.67      0.60         9\n",
      "        347       0.33      0.60      0.43        10\n",
      "        348       0.86      0.75      0.80         8\n",
      "        349       0.71      1.00      0.83         5\n",
      "        350       0.50      0.67      0.57         9\n",
      "        351       0.60      0.50      0.55         6\n",
      "        352       0.88      0.88      0.88         8\n",
      "        353       0.67      0.67      0.67         6\n",
      "        354       0.80      1.00      0.89         4\n",
      "        355       0.83      0.56      0.67         9\n",
      "        356       0.54      0.64      0.58        11\n",
      "        357       0.71      1.00      0.83         5\n",
      "        358       1.00      0.73      0.84        11\n",
      "        359       1.00      0.83      0.91         6\n",
      "        360       0.90      1.00      0.95         9\n",
      "        361       0.88      1.00      0.93         7\n",
      "        362       0.80      0.44      0.57         9\n",
      "        363       0.82      1.00      0.90         9\n",
      "        364       1.00      1.00      1.00         5\n",
      "        365       1.00      0.90      0.95        10\n",
      "        366       0.83      1.00      0.91        10\n",
      "        367       0.64      0.90      0.75        10\n",
      "        368       1.00      0.91      0.95        11\n",
      "        369       0.67      0.57      0.62         7\n",
      "        370       0.82      0.90      0.86        10\n",
      "        371       0.86      0.67      0.75         9\n",
      "        372       0.78      0.58      0.67        12\n",
      "        373       0.60      0.60      0.60         5\n",
      "        374       0.90      0.82      0.86        11\n",
      "        375       0.64      0.88      0.74         8\n",
      "        376       1.00      0.70      0.82        10\n",
      "        377       0.75      0.86      0.80         7\n",
      "        378       0.91      0.83      0.87        12\n",
      "        379       1.00      0.80      0.89        10\n",
      "        380       0.75      0.75      0.75         4\n",
      "        381       0.12      0.25      0.17         4\n",
      "        382       0.50      0.60      0.55        10\n",
      "        383       0.71      0.83      0.77         6\n",
      "        384       0.83      1.00      0.91         5\n",
      "        385       0.00      0.00      0.00         3\n",
      "        386       0.50      0.50      0.50         8\n",
      "        387       0.80      0.80      0.80        10\n",
      "        388       0.88      0.64      0.74        11\n",
      "        389       0.82      0.75      0.78        12\n",
      "        390       0.30      0.38      0.33         8\n",
      "        391       0.70      0.88      0.78         8\n",
      "        392       0.89      1.00      0.94         8\n",
      "        393       0.00      0.00      0.00         2\n",
      "        394       0.42      0.56      0.48         9\n",
      "        395       0.62      0.71      0.67         7\n",
      "        396       0.86      0.67      0.75         9\n",
      "        397       0.77      0.91      0.83        11\n",
      "        398       0.80      0.80      0.80         5\n",
      "        399       1.00      1.00      1.00         3\n",
      "        400       0.75      0.75      0.75         8\n",
      "        401       0.83      0.56      0.67         9\n",
      "        402       1.00      0.57      0.73         7\n",
      "        403       0.78      0.78      0.78         9\n",
      "        404       0.86      0.86      0.86         7\n",
      "        405       0.62      0.83      0.71         6\n",
      "        406       0.75      0.60      0.67         5\n",
      "        407       1.00      0.71      0.83         7\n",
      "        408       1.00      0.75      0.86         8\n",
      "        409       0.50      0.67      0.57         3\n",
      "        410       0.56      0.83      0.67         6\n",
      "        411       1.00      0.56      0.71         9\n",
      "        412       0.00      0.00      0.00         7\n",
      "        413       0.75      0.38      0.50         8\n",
      "        414       0.67      1.00      0.80         4\n",
      "        415       0.80      0.57      0.67         7\n",
      "        416       0.88      1.00      0.93         7\n",
      "        417       0.86      1.00      0.92         6\n",
      "        418       0.50      0.14      0.22         7\n",
      "        419       0.83      0.71      0.77         7\n",
      "        420       0.60      0.75      0.67         4\n",
      "        421       0.78      0.70      0.74        10\n",
      "        422       1.00      0.75      0.86         4\n",
      "        423       0.50      0.25      0.33         8\n",
      "        424       0.17      0.50      0.25         2\n",
      "        425       0.67      0.40      0.50        10\n",
      "        426       1.00      0.86      0.92         7\n",
      "        427       0.50      0.57      0.53         7\n",
      "        428       0.50      0.50      0.50        10\n",
      "        429       0.78      1.00      0.88         7\n",
      "        430       0.90      0.90      0.90        10\n",
      "        431       1.00      0.80      0.89         5\n",
      "        432       0.88      1.00      0.94        15\n",
      "        433       0.89      0.89      0.89         9\n",
      "        434       1.00      0.88      0.93         8\n",
      "        435       0.71      1.00      0.83         5\n",
      "        436       1.00      1.00      1.00         3\n",
      "        437       0.88      0.88      0.88         8\n",
      "        438       1.00      0.91      0.95        11\n",
      "        439       0.75      0.75      0.75         4\n",
      "        440       0.75      0.55      0.63        11\n",
      "        441       0.75      0.90      0.82        10\n",
      "        442       1.00      0.75      0.86         8\n",
      "        443       1.00      0.89      0.94         9\n",
      "        444       1.00      1.00      1.00         5\n",
      "        445       0.92      1.00      0.96        12\n",
      "        446       0.86      1.00      0.92         6\n",
      "        447       1.00      1.00      1.00        11\n",
      "        448       0.67      0.57      0.62         7\n",
      "        449       0.73      1.00      0.84         8\n",
      "        450       0.75      0.86      0.80         7\n",
      "        451       1.00      0.77      0.87        13\n",
      "        452       0.75      0.50      0.60         6\n",
      "        453       0.82      0.90      0.86        10\n",
      "        454       0.62      1.00      0.77         5\n",
      "        455       0.67      0.75      0.71         8\n",
      "        456       0.83      0.62      0.71         8\n",
      "        457       0.83      0.83      0.83         6\n",
      "        458       0.80      0.57      0.67         7\n",
      "        459       0.33      0.33      0.33         9\n",
      "        460       0.83      1.00      0.91         5\n",
      "        461       0.67      1.00      0.80         6\n",
      "        462       0.90      0.75      0.82        12\n",
      "        463       1.00      1.00      1.00        12\n",
      "        464       0.67      0.67      0.67        12\n",
      "        465       1.00      0.89      0.94         9\n",
      "        466       0.62      0.62      0.62         8\n",
      "        467       0.67      0.67      0.67         6\n",
      "        468       0.50      1.00      0.67         4\n",
      "        469       1.00      0.88      0.93         8\n",
      "        470       0.92      0.92      0.92        12\n",
      "        471       1.00      1.00      1.00         3\n",
      "        472       0.71      0.71      0.71         7\n",
      "        473       0.90      0.90      0.90        10\n",
      "        474       0.83      0.83      0.83         6\n",
      "        475       1.00      1.00      1.00         5\n",
      "        476       1.00      1.00      1.00         4\n",
      "        477       0.40      0.50      0.44         8\n",
      "        478       1.00      0.80      0.89        10\n",
      "        479       1.00      0.71      0.83         7\n",
      "        480       1.00      0.67      0.80         6\n",
      "        481       0.78      1.00      0.88         7\n",
      "        482       0.60      1.00      0.75         6\n",
      "        483       0.56      0.45      0.50        11\n",
      "        484       0.60      0.43      0.50         7\n",
      "        485       0.62      0.62      0.62         8\n",
      "        486       1.00      0.86      0.92         7\n",
      "        487       0.78      0.78      0.78         9\n",
      "        488       0.71      0.83      0.77         6\n",
      "        489       1.00      0.86      0.92         7\n",
      "        490       1.00      1.00      1.00        10\n",
      "        491       0.78      0.70      0.74        10\n",
      "        492       0.33      0.50      0.40         2\n",
      "        493       0.67      0.80      0.73         5\n",
      "        494       0.77      0.83      0.80        12\n",
      "        495       1.00      0.75      0.86         8\n",
      "        496       0.93      1.00      0.96        13\n",
      "        497       0.75      1.00      0.86         9\n",
      "        498       0.54      0.88      0.67         8\n",
      "        499       0.88      1.00      0.93         7\n",
      "        500       0.82      1.00      0.90         9\n",
      "        501       0.89      1.00      0.94         8\n",
      "        502       0.62      0.50      0.56        10\n",
      "        503       0.55      0.60      0.57        10\n",
      "        504       0.88      1.00      0.93         7\n",
      "        505       0.83      0.91      0.87        11\n",
      "        506       1.00      1.00      1.00         3\n",
      "        507       0.91      1.00      0.95        10\n",
      "        508       0.86      0.75      0.80         8\n",
      "        509       0.78      1.00      0.88         7\n",
      "        510       1.00      0.80      0.89        10\n",
      "        511       0.64      0.70      0.67        10\n",
      "        512       0.50      0.80      0.62         5\n",
      "        513       1.00      1.00      1.00         8\n",
      "        514       0.80      1.00      0.89         8\n",
      "        515       1.00      0.83      0.91         6\n",
      "        516       0.88      0.88      0.88         8\n",
      "        517       1.00      1.00      1.00         3\n",
      "        518       0.88      0.78      0.82         9\n",
      "        519       0.91      1.00      0.95        10\n",
      "        520       0.67      0.67      0.67         3\n",
      "        521       0.86      1.00      0.92         6\n",
      "        522       1.00      0.86      0.92         7\n",
      "        523       0.38      0.60      0.46         5\n",
      "        524       0.75      1.00      0.86         6\n",
      "        525       1.00      0.75      0.86        12\n",
      "        526       0.50      0.44      0.47         9\n",
      "        527       1.00      1.00      1.00         9\n",
      "        528       1.00      0.45      0.62        11\n",
      "        529       0.83      0.56      0.67         9\n",
      "        530       0.57      0.57      0.57         7\n",
      "        531       0.50      0.62      0.56         8\n",
      "        532       1.00      0.67      0.80         6\n",
      "        533       0.71      0.71      0.71         7\n",
      "        534       0.46      0.60      0.52        10\n",
      "        535       1.00      1.00      1.00         6\n",
      "        536       0.60      0.75      0.67         4\n",
      "\n",
      "avg / total       0.78      0.75      0.75      4296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_ys, model.predict(test_xs_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0 0 ... 0 0 0]\n",
      " [0 8 1 ... 0 0 0]\n",
      " [0 1 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 6 0 0]\n",
      " [0 0 0 ... 0 6 0]\n",
      " [0 0 0 ... 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_ys, model.predict(test_xs_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
